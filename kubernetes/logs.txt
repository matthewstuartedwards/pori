
==> Audit <==
|----------------|-----------------------------------------------------------|----------|---------|---------|---------------------|---------------------|
|    Command     |                           Args                            | Profile  |  User   | Version |     Start Time      |      End Time       |
|----------------|-----------------------------------------------------------|----------|---------|---------|---------------------|---------------------|
| addons         | enable metrics-server                                     | minikube | matthew | v1.33.1 | 07 Oct 24 10:51 MDT | 07 Oct 24 10:51 MDT |
| stop           |                                                           | minikube | matthew | v1.33.1 | 07 Oct 24 13:26 MDT | 07 Oct 24 13:26 MDT |
| start          |                                                           | minikube | matthew | v1.33.1 | 07 Oct 24 13:26 MDT | 07 Oct 24 13:27 MDT |
| mount          | /storage/kubernetesStorage:/storage/kubernetesStorage     | minikube | matthew | v1.33.1 | 07 Oct 24 14:40 MDT |                     |
|                | --port=40000                                              |          |         |         |                     |                     |
| mount          | /storage/kubernetesStorage:/storage/kubernetesStorage     | minikube | matthew | v1.33.1 | 07 Oct 24 14:41 MDT |                     |
|                | --port=40000                                              |          |         |         |                     |                     |
| stop           |                                                           | minikube | matthew | v1.33.1 | 08 Oct 24 12:43 MDT | 08 Oct 24 12:43 MDT |
| start          |                                                           | minikube | matthew | v1.33.1 | 08 Oct 24 12:43 MDT | 08 Oct 24 12:44 MDT |
| dashboard      |                                                           | minikube | matthew | v1.33.1 | 08 Oct 24 12:44 MDT |                     |
| delete         |                                                           | minikube | matthew | v1.33.1 | 08 Oct 24 13:16 MDT | 08 Oct 24 13:17 MDT |
| dashboard      |                                                           | minikube | matthew | v1.33.1 | 08 Oct 24 13:22 MDT |                     |
| start          |                                                           | minikube | matthew | v1.33.1 | 08 Oct 24 13:23 MDT | 08 Oct 24 13:23 MDT |
| dashboard      |                                                           | minikube | matthew | v1.33.1 | 08 Oct 24 13:24 MDT |                     |
| delete         |                                                           | minikube | matthew | v1.33.1 | 08 Oct 24 13:48 MDT | 08 Oct 24 13:48 MDT |
| start          |                                                           | minikube | matthew | v1.33.1 | 08 Oct 24 13:48 MDT | 08 Oct 24 13:49 MDT |
| stop           |                                                           | minikube | matthew | v1.33.1 | 08 Oct 24 13:49 MDT | 08 Oct 24 13:49 MDT |
| update-check   |                                                           | minikube | matthew | v1.33.1 | 08 Oct 24 13:50 MDT | 08 Oct 24 13:50 MDT |
| delete         |                                                           | minikube | matthew | v1.33.1 | 08 Oct 24 13:51 MDT | 08 Oct 24 13:51 MDT |
| __complete     | update-co                                                 | minikube | matthew | v1.33.1 | 08 Oct 24 13:51 MDT | 08 Oct 24 13:51 MDT |
| update-context |                                                           | minikube | matthew | v1.33.1 | 08 Oct 24 13:51 MDT |                     |
| update-check   |                                                           | minikube | matthew | v1.33.1 | 08 Oct 24 13:51 MDT | 08 Oct 24 13:51 MDT |
| __complete     | update-                                                   | minikube | matthew | v1.34.0 | 08 Oct 24 13:53 MDT | 08 Oct 24 13:53 MDT |
| __complete     | update-ch                                                 | minikube | matthew | v1.34.0 | 08 Oct 24 13:53 MDT | 08 Oct 24 13:53 MDT |
| update-check   |                                                           | minikube | matthew | v1.34.0 | 08 Oct 24 13:53 MDT | 08 Oct 24 13:53 MDT |
| addons         | enable ingress                                            | minikube | matthew | v1.34.0 | 08 Oct 24 13:54 MDT |                     |
| start          |                                                           | minikube | matthew | v1.34.0 | 08 Oct 24 13:54 MDT | 08 Oct 24 13:55 MDT |
| addons         | enable ingress                                            | minikube | matthew | v1.34.0 | 08 Oct 24 13:55 MDT | 08 Oct 24 13:55 MDT |
| addons         | enable dashboard                                          | minikube | matthew | v1.34.0 | 08 Oct 24 13:55 MDT | 08 Oct 24 13:55 MDT |
| __complete     | addons enable met                                         | minikube | matthew | v1.34.0 | 08 Oct 24 13:56 MDT | 08 Oct 24 13:56 MDT |
| addons         | enable metrics-server                                     | minikube | matthew | v1.34.0 | 08 Oct 24 13:56 MDT | 08 Oct 24 13:56 MDT |
| addons         | list                                                      | minikube | matthew | v1.34.0 | 08 Oct 24 13:56 MDT | 08 Oct 24 13:56 MDT |
| dashboard      |                                                           | minikube | matthew | v1.34.0 | 08 Oct 24 13:57 MDT |                     |
| docker-env     |                                                           | minikube | matthew | v1.34.0 | 08 Oct 24 14:00 MDT | 08 Oct 24 14:00 MDT |
| tunnel         |                                                           | minikube | matthew | v1.34.0 | 08 Oct 24 14:10 MDT | 08 Oct 24 14:11 MDT |
| mount          | /storage/kubernetesStorage/:/storage/                     | minikube | matthew | v1.34.0 | 08 Oct 24 14:12 MDT |                     |
|                | --port=80                                                 |          |         |         |                     |                     |
| mount          | /storage/kubernetesStorage/:/storage/                     | minikube | matthew | v1.34.0 | 08 Oct 24 14:13 MDT |                     |
|                | --port=40000                                              |          |         |         |                     |                     |
| mount          | /storage/kubernetesStorage/:/orientdb/                    | minikube | matthew | v1.34.0 | 08 Oct 24 14:13 MDT |                     |
|                | --port=40000                                              |          |         |         |                     |                     |
| mount          | /storage/kubernetesStorage/orientdb:/orientdb/            | minikube | matthew | v1.34.0 | 08 Oct 24 15:11 MDT |                     |
|                | --port=40000                                              |          |         |         |                     |                     |
| mount          | /storage/kubernetesStorage/orientdb:/orientdb/            | minikube | matthew | v1.34.0 | 08 Oct 24 15:12 MDT |                     |
|                | --port=40000                                              |          |         |         |                     |                     |
| mount          | /storage/kubernetesStorage/orientdb/:/orientdb/           | minikube | matthew | v1.34.0 | 08 Oct 24 15:12 MDT |                     |
|                | --port=40000                                              |          |         |         |                     |                     |
| stop           |                                                           | minikube | matthew | v1.34.0 | 08 Oct 24 15:24 MDT | 08 Oct 24 15:24 MDT |
| __complete     | start --mount --mount-string                              | minikube | matthew | v1.34.0 | 08 Oct 24 15:24 MDT | 08 Oct 24 15:24 MDT |
|                | "/storage/ku                                              |          |         |         |                     |                     |
| start          | --mount --mount-string                                    | minikube | matthew | v1.34.0 | 08 Oct 24 15:26 MDT |                     |
|                | /storage/kubernetesStorage/:/orientdb/                    |          |         |         |                     |                     |
| delete         |                                                           | minikube | matthew | v1.34.0 | 08 Oct 24 15:27 MDT | 08 Oct 24 15:27 MDT |
| start          | --mount --mount-string                                    | minikube | matthew | v1.34.0 | 08 Oct 24 15:27 MDT | 08 Oct 24 15:28 MDT |
|                | /storage/kubernetesStorage/:/orientdb/                    |          |         |         |                     |                     |
| dashboard      |                                                           | minikube | matthew | v1.34.0 | 08 Oct 24 15:33 MDT |                     |
| docker-env     |                                                           | minikube | matthew | v1.34.0 | 08 Oct 24 15:37 MDT | 08 Oct 24 15:37 MDT |
| docker-env     |                                                           | minikube | matthew | v1.34.0 | 08 Oct 24 15:39 MDT | 08 Oct 24 15:39 MDT |
| docker-env     |                                                           | minikube | matthew | v1.34.0 | 08 Oct 24 15:40 MDT | 08 Oct 24 15:40 MDT |
| docker-env     |                                                           | minikube | matthew | v1.34.0 | 08 Oct 24 15:40 MDT | 08 Oct 24 15:40 MDT |
| update-check   |                                                           | minikube | matthew | v1.34.0 | 10 Oct 24 08:45 MDT | 10 Oct 24 08:45 MDT |
| __complete     | mou                                                       | minikube | matthew | v1.34.0 | 10 Oct 24 08:55 MDT | 10 Oct 24 08:55 MDT |
| mount          | /storage/kubernetesStorage/orientdb/:/orientdb/           | minikube | matthew | v1.34.0 | 10 Oct 24 08:55 MDT |                     |
|                | --port=40000                                              |          |         |         |                     |                     |
| mount          | /storage/kubernetesStorage/orientdb/:/orientdb/databases/ | minikube | matthew | v1.34.0 | 10 Oct 24 09:06 MDT |                     |
|                | --port=40000                                              |          |         |         |                     |                     |
| mount          | /storage/kubernetesStorage/orientdb/:/orientdb/databases/ | minikube | matthew | v1.34.0 | 10 Oct 24 09:07 MDT |                     |
|                | --port=40000                                              |          |         |         |                     |                     |
| mount          |                                                           | minikube | matthew | v1.34.0 | 10 Oct 24 09:08 MDT |                     |
| __complete     | unmo                                                      | minikube | matthew | v1.34.0 | 10 Oct 24 09:08 MDT | 10 Oct 24 09:08 MDT |
| __complete     | unmo                                                      | minikube | matthew | v1.34.0 | 10 Oct 24 09:08 MDT | 10 Oct 24 09:08 MDT |
| __complete     | unmo                                                      | minikube | matthew | v1.34.0 | 10 Oct 24 09:08 MDT | 10 Oct 24 09:08 MDT |
| mount          | /storage/kubernetesStorage/orientdb/:/orientdb/databases/ | minikube | matthew | v1.34.0 | 10 Oct 24 09:08 MDT |                     |
|                | --port=40000                                              |          |         |         |                     |                     |
| mount          | /storage/kubernetesStorage/orientdb/:/orientdb/           | minikube | matthew | v1.34.0 | 10 Oct 24 09:12 MDT |                     |
|                | --port=40000                                              |          |         |         |                     |                     |
|----------------|-----------------------------------------------------------|----------|---------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/10/08 15:27:59
Running on machine: Azaghal
Binary: Built with gc go1.22.5 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1008 15:27:59.272377 2386321 out.go:345] Setting OutFile to fd 1 ...
I1008 15:27:59.272544 2386321 out.go:397] isatty.IsTerminal(1) = true
I1008 15:27:59.272545 2386321 out.go:358] Setting ErrFile to fd 2...
I1008 15:27:59.272547 2386321 out.go:397] isatty.IsTerminal(2) = true
I1008 15:27:59.272652 2386321 root.go:338] Updating PATH: /home/matthew/.minikube/bin
I1008 15:27:59.272904 2386321 out.go:352] Setting JSON to false
I1008 15:27:59.274845 2386321 start.go:129] hostinfo: {"hostname":"Azaghal","uptime":110028,"bootTime":1728312852,"procs":725,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"6.8.0-45-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"f6ba9b9f-3056-4118-be33-0985abb4df7c"}
I1008 15:27:59.274869 2386321 start.go:139] virtualization: kvm host
I1008 15:27:59.282442 2386321 out.go:177] 😄  minikube v1.34.0 on Ubuntu 24.04
I1008 15:27:59.286297 2386321 notify.go:220] Checking for updates...
I1008 15:27:59.286466 2386321 config.go:182] Loaded profile config "cluster2": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I1008 15:27:59.286514 2386321 driver.go:394] Setting default libvirt URI to qemu:///system
I1008 15:27:59.286525 2386321 global.go:112] Querying for installed drivers using PATH=/home/matthew/.minikube/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin
I1008 15:27:59.286558 2386321 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I1008 15:27:59.299784 2386321 docker.go:123] docker version: linux-27.3.1:Docker Engine - Community
I1008 15:27:59.299838 2386321 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1008 15:27:59.358101 2386321 info.go:266] docker info: {ID:5d32db62-168d-4c7e-9dc2-c56b229751da Containers:66 ContainersRunning:5 ContainersPaused:0 ContainersStopped:61 Images:65 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:63 OomKillDisable:false NGoroutines:85 SystemTime:2024-10-08 15:27:59.35230978 -0600 MDT LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-45-generic OperatingSystem:Ubuntu 24.04.1 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:24 MemTotal:66584821760 GenericResources:<nil> DockerRootDir:/storage/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Azaghal Labels:[] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c Expected:7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c} RuncCommit:{ID:v1.1.14-0-g2c9f560 Expected:v1.1.14-0-g2c9f560} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:[WARNING: bridge-nf-call-iptables is disabled WARNING: bridge-nf-call-ip6tables is disabled] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.17.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.7]] Warnings:<nil>}}
I1008 15:27:59.358151 2386321 docker.go:318] overlay module found
I1008 15:27:59.358159 2386321 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1008 15:27:59.362066 2386321 global.go:133] none default: false priority: 4, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:running the 'none' driver as a regular user requires sudo permissions Reason: Fix: Doc: Version:}
I1008 15:27:59.362109 2386321 global.go:133] podman default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I1008 15:27:59.362118 2386321 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1008 15:27:59.362149 2386321 global.go:133] kvm2 default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "virsh": executable file not found in $PATH Reason: Fix:Install libvirt Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/kvm2/ Version:}
I1008 15:27:59.362173 2386321 global.go:133] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I1008 15:27:59.396426 2386321 virtualbox.go:136] virtual box version: 7.0.16_Ubuntur162802
I1008 15:27:59.396439 2386321 global.go:133] virtualbox default: true priority: 6, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:7.0.16_Ubuntur162802
}
I1008 15:27:59.396451 2386321 driver.go:316] not recommending "ssh" due to default: false
I1008 15:27:59.396461 2386321 driver.go:351] Picked: docker
I1008 15:27:59.396464 2386321 driver.go:352] Alternatives: [virtualbox ssh]
I1008 15:27:59.396466 2386321 driver.go:353] Rejects: [vmware none podman kvm2 qemu2]
I1008 15:27:59.400905 2386321 out.go:177] ✨  Automatically selected the docker driver. Other choices: virtualbox, ssh
I1008 15:27:59.404615 2386321 start.go:297] selected driver: docker
I1008 15:27:59.404618 2386321 start.go:901] validating driver "docker" against <nil>
I1008 15:27:59.404623 2386321 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1008 15:27:59.404666 2386321 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1008 15:27:59.429866 2386321 info.go:266] docker info: {ID:5d32db62-168d-4c7e-9dc2-c56b229751da Containers:66 ContainersRunning:5 ContainersPaused:0 ContainersStopped:61 Images:65 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:63 OomKillDisable:false NGoroutines:85 SystemTime:2024-10-08 15:27:59.424348717 -0600 MDT LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-45-generic OperatingSystem:Ubuntu 24.04.1 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:24 MemTotal:66584821760 GenericResources:<nil> DockerRootDir:/storage/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Azaghal Labels:[] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c Expected:7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c} RuncCommit:{ID:v1.1.14-0-g2c9f560 Expected:v1.1.14-0-g2c9f560} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:[WARNING: bridge-nf-call-iptables is disabled WARNING: bridge-nf-call-ip6tables is disabled] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.17.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.7]] Warnings:<nil>}}
I1008 15:27:59.429935 2386321 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I1008 15:27:59.431001 2386321 start_flags.go:393] Using suggested 15800MB memory alloc based on sys=63500MB, container=63500MB
I1008 15:27:59.431073 2386321 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I1008 15:27:59.435405 2386321 out.go:177] 📌  Using Docker driver with root privileges
I1008 15:27:59.439106 2386321 cni.go:84] Creating CNI manager for ""
I1008 15:27:59.439113 2386321 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1008 15:27:59.439117 2386321 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I1008 15:27:59.439148 2386321 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:15800 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[/storage/kubernetesStorage/:/orientdb/] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:true MountString:/storage/kubernetesStorage/:/orientdb/ Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1008 15:27:59.442899 2386321 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I1008 15:27:59.450307 2386321 cache.go:121] Beginning downloading kic base image for docker with docker
I1008 15:27:59.454107 2386321 out.go:177] 🚜  Pulling base image v0.0.45 ...
I1008 15:27:59.457907 2386321 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1008 15:27:59.457923 2386321 preload.go:146] Found local preload: /home/matthew/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I1008 15:27:59.457926 2386321 cache.go:56] Caching tarball of preloaded images
I1008 15:27:59.457950 2386321 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I1008 15:27:59.457971 2386321 preload.go:172] Found /home/matthew/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1008 15:27:59.457982 2386321 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I1008 15:27:59.458038 2386321 profile.go:143] Saving config to /home/matthew/.minikube/profiles/minikube/config.json ...
I1008 15:27:59.458047 2386321 lock.go:35] WriteFile acquiring /home/matthew/.minikube/profiles/minikube/config.json: {Name:mk970e7f2e6c1b69b85b1739010d0d57d580c724 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
W1008 15:27:59.473145 2386321 image.go:95] image gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 is of wrong architecture
I1008 15:27:59.473149 2386321 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I1008 15:27:59.473185 2386321 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I1008 15:27:59.473191 2386321 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I1008 15:27:59.473193 2386321 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I1008 15:27:59.473196 2386321 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I1008 15:27:59.473198 2386321 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I1008 15:27:59.601664 2386321 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I1008 15:27:59.601677 2386321 cache.go:194] Successfully downloaded all kic artifacts
I1008 15:27:59.601693 2386321 start.go:360] acquireMachinesLock for minikube: {Name:mka5934feaf0cbac8f088ff0c2c00344582042d0 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1008 15:27:59.601733 2386321 start.go:364] duration metric: took 31.48µs to acquireMachinesLock for "minikube"
I1008 15:27:59.601744 2386321 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:15800 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[/storage/kubernetesStorage/:/orientdb/] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:true MountString:/storage/kubernetesStorage/:/orientdb/ Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1008 15:27:59.601777 2386321 start.go:125] createHost starting for "" (driver="docker")
I1008 15:27:59.619983 2386321 out.go:235] 🔥  Creating docker container (CPUs=2, Memory=15800MB) ...
I1008 15:27:59.620151 2386321 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1008 15:27:59.620164 2386321 client.go:168] LocalClient.Create starting
I1008 15:27:59.620187 2386321 main.go:141] libmachine: Reading certificate data from /home/matthew/.minikube/certs/ca.pem
I1008 15:27:59.620202 2386321 main.go:141] libmachine: Decoding PEM data...
I1008 15:27:59.620207 2386321 main.go:141] libmachine: Parsing certificate...
I1008 15:27:59.620232 2386321 main.go:141] libmachine: Reading certificate data from /home/matthew/.minikube/certs/cert.pem
I1008 15:27:59.620244 2386321 main.go:141] libmachine: Decoding PEM data...
I1008 15:27:59.620251 2386321 main.go:141] libmachine: Parsing certificate...
I1008 15:27:59.620443 2386321 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1008 15:27:59.629548 2386321 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1008 15:27:59.629573 2386321 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I1008 15:27:59.629580 2386321 cli_runner.go:164] Run: docker network inspect minikube
W1008 15:27:59.638400 2386321 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1008 15:27:59.638405 2386321 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I1008 15:27:59.638408 2386321 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I1008 15:27:59.638444 2386321 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1008 15:27:59.647977 2386321 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc001b0a610}
I1008 15:27:59.647990 2386321 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I1008 15:27:59.648016 2386321 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I1008 15:27:59.819503 2386321 network_create.go:108] docker network minikube 192.168.49.0/24 created
I1008 15:27:59.819515 2386321 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I1008 15:27:59.819568 2386321 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1008 15:27:59.834356 2386321 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1008 15:27:59.881929 2386321 oci.go:103] Successfully created a docker volume minikube
I1008 15:27:59.881969 2386321 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -d /var/lib
I1008 15:28:01.868940 2386321 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -d /var/lib: (1.98695283s)
I1008 15:28:01.868961 2386321 oci.go:107] Successfully prepared a docker volume minikube
I1008 15:28:01.868986 2386321 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1008 15:28:01.868996 2386321 kic.go:194] Starting extracting preloaded images to volume ...
I1008 15:28:01.869033 2386321 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/matthew/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -I lz4 -xf /preloaded.tar -C /extractDir
I1008 15:28:08.268977 2386321 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/matthew/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -I lz4 -xf /preloaded.tar -C /extractDir: (6.3999235s)
I1008 15:28:08.268988 2386321 kic.go:203] duration metric: took 6.399989709s to extract preloaded images to volume ...
W1008 15:28:08.269050 2386321 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W1008 15:28:08.269064 2386321 oci.go:243] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I1008 15:28:08.269089 2386321 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1008 15:28:08.294129 2386321 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=15800mb -e container=docker --expose 8443 --volume=/storage/kubernetesStorage/:/orientdb/ --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85
I1008 15:28:09.288950 2386321 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I1008 15:28:09.298588 2386321 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1008 15:28:09.308114 2386321 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1008 15:28:09.347288 2386321 oci.go:144] the created container "minikube" has a running status.
I1008 15:28:09.347305 2386321 kic.go:225] Creating ssh key for kic: /home/matthew/.minikube/machines/minikube/id_rsa...
I1008 15:28:09.401328 2386321 kic_runner.go:191] docker (temp): /home/matthew/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1008 15:28:09.413905 2386321 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1008 15:28:09.423334 2386321 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1008 15:28:09.423343 2386321 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1008 15:28:09.453021 2386321 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1008 15:28:09.462047 2386321 machine.go:93] provisionDockerMachine start ...
I1008 15:28:09.462080 2386321 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1008 15:28:09.472871 2386321 main.go:141] libmachine: Using SSH client type: native
I1008 15:28:09.472973 2386321 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32808 <nil> <nil>}
I1008 15:28:09.472976 2386321 main.go:141] libmachine: About to run SSH command:
hostname
I1008 15:28:09.473414 2386321 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:60988->127.0.0.1:32808: read: connection reset by peer
I1008 15:28:12.578748 2386321 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1008 15:28:12.578759 2386321 ubuntu.go:169] provisioning hostname "minikube"
I1008 15:28:12.578787 2386321 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1008 15:28:12.588555 2386321 main.go:141] libmachine: Using SSH client type: native
I1008 15:28:12.588635 2386321 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32808 <nil> <nil>}
I1008 15:28:12.588637 2386321 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1008 15:28:12.701621 2386321 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1008 15:28:12.701653 2386321 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1008 15:28:12.711875 2386321 main.go:141] libmachine: Using SSH client type: native
I1008 15:28:12.711952 2386321 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32808 <nil> <nil>}
I1008 15:28:12.711957 2386321 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1008 15:28:12.816013 2386321 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1008 15:28:12.816023 2386321 ubuntu.go:175] set auth options {CertDir:/home/matthew/.minikube CaCertPath:/home/matthew/.minikube/certs/ca.pem CaPrivateKeyPath:/home/matthew/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/matthew/.minikube/machines/server.pem ServerKeyPath:/home/matthew/.minikube/machines/server-key.pem ClientKeyPath:/home/matthew/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/matthew/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/matthew/.minikube}
I1008 15:28:12.816031 2386321 ubuntu.go:177] setting up certificates
I1008 15:28:12.816035 2386321 provision.go:84] configureAuth start
I1008 15:28:12.816063 2386321 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1008 15:28:12.826301 2386321 provision.go:143] copyHostCerts
I1008 15:28:12.826323 2386321 exec_runner.go:144] found /home/matthew/.minikube/ca.pem, removing ...
I1008 15:28:12.826325 2386321 exec_runner.go:203] rm: /home/matthew/.minikube/ca.pem
I1008 15:28:12.826368 2386321 exec_runner.go:151] cp: /home/matthew/.minikube/certs/ca.pem --> /home/matthew/.minikube/ca.pem (1078 bytes)
I1008 15:28:12.826426 2386321 exec_runner.go:144] found /home/matthew/.minikube/cert.pem, removing ...
I1008 15:28:12.826427 2386321 exec_runner.go:203] rm: /home/matthew/.minikube/cert.pem
I1008 15:28:12.826447 2386321 exec_runner.go:151] cp: /home/matthew/.minikube/certs/cert.pem --> /home/matthew/.minikube/cert.pem (1123 bytes)
I1008 15:28:12.826482 2386321 exec_runner.go:144] found /home/matthew/.minikube/key.pem, removing ...
I1008 15:28:12.826484 2386321 exec_runner.go:203] rm: /home/matthew/.minikube/key.pem
I1008 15:28:12.826502 2386321 exec_runner.go:151] cp: /home/matthew/.minikube/certs/key.pem --> /home/matthew/.minikube/key.pem (1675 bytes)
I1008 15:28:12.826536 2386321 provision.go:117] generating server cert: /home/matthew/.minikube/machines/server.pem ca-key=/home/matthew/.minikube/certs/ca.pem private-key=/home/matthew/.minikube/certs/ca-key.pem org=matthew.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1008 15:28:12.980588 2386321 provision.go:177] copyRemoteCerts
I1008 15:28:12.980612 2386321 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1008 15:28:12.980633 2386321 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1008 15:28:12.991283 2386321 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32808 SSHKeyPath:/home/matthew/.minikube/machines/minikube/id_rsa Username:docker}
I1008 15:28:13.069424 2386321 ssh_runner.go:362] scp /home/matthew/.minikube/machines/server.pem --> /etc/docker/server.pem (1180 bytes)
I1008 15:28:13.084880 2386321 ssh_runner.go:362] scp /home/matthew/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1008 15:28:13.099910 2386321 ssh_runner.go:362] scp /home/matthew/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1008 15:28:13.115111 2386321 provision.go:87] duration metric: took 299.071912ms to configureAuth
I1008 15:28:13.115117 2386321 ubuntu.go:193] setting minikube options for container-runtime
I1008 15:28:13.115193 2386321 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1008 15:28:13.115215 2386321 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1008 15:28:13.125527 2386321 main.go:141] libmachine: Using SSH client type: native
I1008 15:28:13.125609 2386321 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32808 <nil> <nil>}
I1008 15:28:13.125612 2386321 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1008 15:28:13.230329 2386321 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1008 15:28:13.230334 2386321 ubuntu.go:71] root file system type: overlay
I1008 15:28:13.230383 2386321 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1008 15:28:13.230416 2386321 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1008 15:28:13.240351 2386321 main.go:141] libmachine: Using SSH client type: native
I1008 15:28:13.240435 2386321 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32808 <nil> <nil>}
I1008 15:28:13.240461 2386321 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1008 15:28:13.352639 2386321 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1008 15:28:13.352673 2386321 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1008 15:28:13.363079 2386321 main.go:141] libmachine: Using SSH client type: native
I1008 15:28:13.363156 2386321 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32808 <nil> <nil>}
I1008 15:28:13.363162 2386321 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1008 15:28:14.561114 2386321 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-08-27 14:13:43.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-10-08 21:28:13.351136029 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1008 15:28:14.561126 2386321 machine.go:96] duration metric: took 5.099073807s to provisionDockerMachine
I1008 15:28:14.561134 2386321 client.go:171] duration metric: took 14.940966765s to LocalClient.Create
I1008 15:28:14.561141 2386321 start.go:167] duration metric: took 14.940988354s to libmachine.API.Create "minikube"
I1008 15:28:14.561144 2386321 start.go:293] postStartSetup for "minikube" (driver="docker")
I1008 15:28:14.561149 2386321 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1008 15:28:14.561180 2386321 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1008 15:28:14.561200 2386321 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1008 15:28:14.571661 2386321 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32808 SSHKeyPath:/home/matthew/.minikube/machines/minikube/id_rsa Username:docker}
I1008 15:28:14.649802 2386321 ssh_runner.go:195] Run: cat /etc/os-release
I1008 15:28:14.651809 2386321 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1008 15:28:14.651825 2386321 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1008 15:28:14.651828 2386321 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1008 15:28:14.651831 2386321 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I1008 15:28:14.651834 2386321 filesync.go:126] Scanning /home/matthew/.minikube/addons for local assets ...
I1008 15:28:14.651862 2386321 filesync.go:126] Scanning /home/matthew/.minikube/files for local assets ...
I1008 15:28:14.651873 2386321 start.go:296] duration metric: took 90.726546ms for postStartSetup
I1008 15:28:14.652042 2386321 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1008 15:28:14.662388 2386321 profile.go:143] Saving config to /home/matthew/.minikube/profiles/minikube/config.json ...
I1008 15:28:14.662548 2386321 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1008 15:28:14.662567 2386321 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1008 15:28:14.671814 2386321 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32808 SSHKeyPath:/home/matthew/.minikube/machines/minikube/id_rsa Username:docker}
I1008 15:28:14.746529 2386321 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1008 15:28:14.749336 2386321 start.go:128] duration metric: took 15.147553596s to createHost
I1008 15:28:14.749341 2386321 start.go:83] releasing machines lock for "minikube", held for 15.147605556s
I1008 15:28:14.749369 2386321 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1008 15:28:14.759179 2386321 ssh_runner.go:195] Run: cat /version.json
I1008 15:28:14.759196 2386321 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1008 15:28:14.759235 2386321 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1008 15:28:14.759269 2386321 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1008 15:28:14.768990 2386321 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32808 SSHKeyPath:/home/matthew/.minikube/machines/minikube/id_rsa Username:docker}
I1008 15:28:14.769309 2386321 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32808 SSHKeyPath:/home/matthew/.minikube/machines/minikube/id_rsa Username:docker}
I1008 15:28:14.843855 2386321 ssh_runner.go:195] Run: systemctl --version
I1008 15:28:14.921705 2386321 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1008 15:28:14.924665 2386321 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1008 15:28:14.941476 2386321 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1008 15:28:14.941499 2386321 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1008 15:28:15.029692 2386321 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I1008 15:28:15.029702 2386321 start.go:495] detecting cgroup driver to use...
I1008 15:28:15.029721 2386321 detect.go:190] detected "systemd" cgroup driver on host os
I1008 15:28:15.029785 2386321 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1008 15:28:15.041381 2386321 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1008 15:28:15.048256 2386321 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1008 15:28:15.054327 2386321 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I1008 15:28:15.054382 2386321 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I1008 15:28:15.061107 2386321 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1008 15:28:15.067833 2386321 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1008 15:28:15.074356 2386321 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1008 15:28:15.080722 2386321 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1008 15:28:15.086181 2386321 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1008 15:28:15.092516 2386321 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1008 15:28:15.098719 2386321 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1008 15:28:15.105047 2386321 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1008 15:28:15.110724 2386321 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1008 15:28:15.116004 2386321 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1008 15:28:15.157713 2386321 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1008 15:28:15.223872 2386321 start.go:495] detecting cgroup driver to use...
I1008 15:28:15.223891 2386321 detect.go:190] detected "systemd" cgroup driver on host os
I1008 15:28:15.223914 2386321 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1008 15:28:15.242264 2386321 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1008 15:28:15.242280 2386321 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1008 15:28:15.249510 2386321 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1008 15:28:15.259299 2386321 ssh_runner.go:195] Run: which cri-dockerd
I1008 15:28:15.261176 2386321 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1008 15:28:15.266165 2386321 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I1008 15:28:15.278036 2386321 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1008 15:28:15.327142 2386321 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1008 15:28:15.369271 2386321 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I1008 15:28:15.369341 2386321 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I1008 15:28:15.382222 2386321 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1008 15:28:15.424297 2386321 ssh_runner.go:195] Run: sudo systemctl restart docker
I1008 15:28:17.118340 2386321 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.6940311s)
I1008 15:28:17.118367 2386321 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1008 15:28:17.126336 2386321 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1008 15:28:17.133569 2386321 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1008 15:28:17.178972 2386321 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1008 15:28:17.221609 2386321 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1008 15:28:17.263263 2386321 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1008 15:28:17.284054 2386321 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1008 15:28:17.291250 2386321 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1008 15:28:17.335873 2386321 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1008 15:28:17.389685 2386321 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1008 15:28:17.389716 2386321 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1008 15:28:17.391879 2386321 start.go:563] Will wait 60s for crictl version
I1008 15:28:17.391896 2386321 ssh_runner.go:195] Run: which crictl
I1008 15:28:17.393949 2386321 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1008 15:28:17.413166 2386321 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I1008 15:28:17.413184 2386321 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1008 15:28:17.428198 2386321 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1008 15:28:17.450091 2386321 out.go:235] 🐳  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I1008 15:28:17.450150 2386321 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1008 15:28:17.460157 2386321 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1008 15:28:17.462521 2386321 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1008 15:28:17.470225 2386321 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:15800 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[/storage/kubernetesStorage/:/orientdb/] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:true MountString:/storage/kubernetesStorage/:/orientdb/ Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1008 15:28:17.470289 2386321 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1008 15:28:17.470318 2386321 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1008 15:28:17.482150 2386321 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1008 15:28:17.482157 2386321 docker.go:615] Images already preloaded, skipping extraction
I1008 15:28:17.482186 2386321 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1008 15:28:17.493133 2386321 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1008 15:28:17.493139 2386321 cache_images.go:84] Images are preloaded, skipping loading
I1008 15:28:17.493148 2386321 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I1008 15:28:17.493209 2386321 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1008 15:28:17.493232 2386321 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1008 15:28:17.521232 2386321 cni.go:84] Creating CNI manager for ""
I1008 15:28:17.521248 2386321 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1008 15:28:17.521259 2386321 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1008 15:28:17.521268 2386321 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1008 15:28:17.521340 2386321 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1008 15:28:17.521369 2386321 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1008 15:28:17.527581 2386321 binaries.go:44] Found k8s binaries, skipping transfer
I1008 15:28:17.527603 2386321 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1008 15:28:17.533596 2386321 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1008 15:28:17.545092 2386321 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1008 15:28:17.556157 2386321 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2149 bytes)
I1008 15:28:17.567111 2386321 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1008 15:28:17.569144 2386321 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1008 15:28:17.575732 2386321 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1008 15:28:17.617442 2386321 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1008 15:28:17.633535 2386321 certs.go:68] Setting up /home/matthew/.minikube/profiles/minikube for IP: 192.168.49.2
I1008 15:28:17.633539 2386321 certs.go:194] generating shared ca certs ...
I1008 15:28:17.633545 2386321 certs.go:226] acquiring lock for ca certs: {Name:mk66277f32e579a583b9c52abedc8a520b85a3a2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1008 15:28:17.633613 2386321 certs.go:235] skipping valid "minikubeCA" ca cert: /home/matthew/.minikube/ca.key
I1008 15:28:17.633631 2386321 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/matthew/.minikube/proxy-client-ca.key
I1008 15:28:17.633635 2386321 certs.go:256] generating profile certs ...
I1008 15:28:17.633660 2386321 certs.go:363] generating signed profile cert for "minikube-user": /home/matthew/.minikube/profiles/minikube/client.key
I1008 15:28:17.633665 2386321 crypto.go:68] Generating cert /home/matthew/.minikube/profiles/minikube/client.crt with IP's: []
I1008 15:28:17.705920 2386321 crypto.go:156] Writing cert to /home/matthew/.minikube/profiles/minikube/client.crt ...
I1008 15:28:17.705930 2386321 lock.go:35] WriteFile acquiring /home/matthew/.minikube/profiles/minikube/client.crt: {Name:mk60ca11178fcd32a5496699fb5c72286d5de5cc Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1008 15:28:17.706020 2386321 crypto.go:164] Writing key to /home/matthew/.minikube/profiles/minikube/client.key ...
I1008 15:28:17.706022 2386321 lock.go:35] WriteFile acquiring /home/matthew/.minikube/profiles/minikube/client.key: {Name:mkcd87384ca54b58393f74bd03378119558295dd Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1008 15:28:17.706062 2386321 certs.go:363] generating signed profile cert for "minikube": /home/matthew/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1008 15:28:17.706069 2386321 crypto.go:68] Generating cert /home/matthew/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I1008 15:28:17.768313 2386321 crypto.go:156] Writing cert to /home/matthew/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I1008 15:28:17.768319 2386321 lock.go:35] WriteFile acquiring /home/matthew/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk149212cf53f9325502b9e8a8ef7183c4ab7586 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1008 15:28:17.768381 2386321 crypto.go:164] Writing key to /home/matthew/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I1008 15:28:17.768383 2386321 lock.go:35] WriteFile acquiring /home/matthew/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk858509977c68b0f12b7261b77d690189a4e982 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1008 15:28:17.768428 2386321 certs.go:381] copying /home/matthew/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/matthew/.minikube/profiles/minikube/apiserver.crt
I1008 15:28:17.768470 2386321 certs.go:385] copying /home/matthew/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/matthew/.minikube/profiles/minikube/apiserver.key
I1008 15:28:17.768503 2386321 certs.go:363] generating signed profile cert for "aggregator": /home/matthew/.minikube/profiles/minikube/proxy-client.key
I1008 15:28:17.768510 2386321 crypto.go:68] Generating cert /home/matthew/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I1008 15:28:17.970309 2386321 crypto.go:156] Writing cert to /home/matthew/.minikube/profiles/minikube/proxy-client.crt ...
I1008 15:28:17.970317 2386321 lock.go:35] WriteFile acquiring /home/matthew/.minikube/profiles/minikube/proxy-client.crt: {Name:mk74a142e3fd212a7f60ad3b8ac941b73737a72b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1008 15:28:17.970404 2386321 crypto.go:164] Writing key to /home/matthew/.minikube/profiles/minikube/proxy-client.key ...
I1008 15:28:17.970406 2386321 lock.go:35] WriteFile acquiring /home/matthew/.minikube/profiles/minikube/proxy-client.key: {Name:mkecfee1fc455157a7f5657ea5c2b7c64ce06a16 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1008 15:28:17.970500 2386321 certs.go:484] found cert: /home/matthew/.minikube/certs/ca-key.pem (1675 bytes)
I1008 15:28:17.970516 2386321 certs.go:484] found cert: /home/matthew/.minikube/certs/ca.pem (1078 bytes)
I1008 15:28:17.970528 2386321 certs.go:484] found cert: /home/matthew/.minikube/certs/cert.pem (1123 bytes)
I1008 15:28:17.970538 2386321 certs.go:484] found cert: /home/matthew/.minikube/certs/key.pem (1675 bytes)
I1008 15:28:17.970833 2386321 ssh_runner.go:362] scp /home/matthew/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1008 15:28:17.986877 2386321 ssh_runner.go:362] scp /home/matthew/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1008 15:28:18.001903 2386321 ssh_runner.go:362] scp /home/matthew/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1008 15:28:18.016093 2386321 ssh_runner.go:362] scp /home/matthew/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1008 15:28:18.030369 2386321 ssh_runner.go:362] scp /home/matthew/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1008 15:28:18.044508 2386321 ssh_runner.go:362] scp /home/matthew/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1008 15:28:18.058881 2386321 ssh_runner.go:362] scp /home/matthew/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1008 15:28:18.073387 2386321 ssh_runner.go:362] scp /home/matthew/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1008 15:28:18.086895 2386321 ssh_runner.go:362] scp /home/matthew/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1008 15:28:18.100798 2386321 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1008 15:28:18.111018 2386321 ssh_runner.go:195] Run: openssl version
I1008 15:28:18.113986 2386321 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1008 15:28:18.119303 2386321 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1008 15:28:18.121131 2386321 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Aug 21 16:05 /usr/share/ca-certificates/minikubeCA.pem
I1008 15:28:18.121149 2386321 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1008 15:28:18.124555 2386321 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1008 15:28:18.130352 2386321 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1008 15:28:18.132281 2386321 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I1008 15:28:18.132298 2386321 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:15800 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[/storage/kubernetesStorage/:/orientdb/] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:true MountString:/storage/kubernetesStorage/:/orientdb/ Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1008 15:28:18.132343 2386321 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1008 15:28:18.142364 2386321 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1008 15:28:18.147649 2386321 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1008 15:28:18.152851 2386321 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I1008 15:28:18.152871 2386321 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1008 15:28:18.158733 2386321 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1008 15:28:18.158738 2386321 kubeadm.go:157] found existing configuration files:

I1008 15:28:18.158755 2386321 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1008 15:28:18.163860 2386321 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I1008 15:28:18.163877 2386321 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I1008 15:28:18.168741 2386321 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1008 15:28:18.173564 2386321 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I1008 15:28:18.173582 2386321 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I1008 15:28:18.178324 2386321 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1008 15:28:18.183619 2386321 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I1008 15:28:18.183634 2386321 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1008 15:28:18.188211 2386321 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1008 15:28:18.193495 2386321 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I1008 15:28:18.193514 2386321 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1008 15:28:18.198418 2386321 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1008 15:28:18.215596 2386321 kubeadm.go:310] W1008 21:28:18.215404    1886 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I1008 15:28:18.215832 2386321 kubeadm.go:310] W1008 21:28:18.215686    1886 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "InitConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I1008 15:28:18.224004 2386321 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I1008 15:28:18.226009 2386321 kubeadm.go:310] 	[WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "modprobe: FATAL: Module configs not found in directory /lib/modules/6.8.0-45-generic\n", err: exit status 1
I1008 15:28:18.254900 2386321 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1008 15:28:30.734526 2386321 kubeadm.go:310] [init] Using Kubernetes version: v1.31.0
I1008 15:28:30.734546 2386321 kubeadm.go:310] [preflight] Running pre-flight checks
I1008 15:28:30.734580 2386321 kubeadm.go:310] [preflight] The system verification failed. Printing the output from the verification:
I1008 15:28:30.734605 2386321 kubeadm.go:310] [0;37mKERNEL_VERSION[0m: [0;32m6.8.0-45-generic[0m
I1008 15:28:30.734618 2386321 kubeadm.go:310] [0;37mOS[0m: [0;32mLinux[0m
I1008 15:28:30.734634 2386321 kubeadm.go:310] [0;37mCGROUPS_CPU[0m: [0;32menabled[0m
I1008 15:28:30.734650 2386321 kubeadm.go:310] [0;37mCGROUPS_CPUSET[0m: [0;32menabled[0m
I1008 15:28:30.734671 2386321 kubeadm.go:310] [0;37mCGROUPS_DEVICES[0m: [0;32menabled[0m
I1008 15:28:30.734688 2386321 kubeadm.go:310] [0;37mCGROUPS_FREEZER[0m: [0;32menabled[0m
I1008 15:28:30.734704 2386321 kubeadm.go:310] [0;37mCGROUPS_MEMORY[0m: [0;32menabled[0m
I1008 15:28:30.734723 2386321 kubeadm.go:310] [0;37mCGROUPS_PIDS[0m: [0;32menabled[0m
I1008 15:28:30.734740 2386321 kubeadm.go:310] [0;37mCGROUPS_HUGETLB[0m: [0;32menabled[0m
I1008 15:28:30.734755 2386321 kubeadm.go:310] [0;37mCGROUPS_IO[0m: [0;32menabled[0m
I1008 15:28:30.734784 2386321 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I1008 15:28:30.734817 2386321 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1008 15:28:30.734851 2386321 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I1008 15:28:30.734873 2386321 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1008 15:28:30.742410 2386321 out.go:235]     ▪ Generating certificates and keys ...
I1008 15:28:30.742463 2386321 kubeadm.go:310] [certs] Using existing ca certificate authority
I1008 15:28:30.742486 2386321 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I1008 15:28:30.742515 2386321 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I1008 15:28:30.742538 2386321 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I1008 15:28:30.742559 2386321 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I1008 15:28:30.742577 2386321 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I1008 15:28:30.742596 2386321 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I1008 15:28:30.742634 2386321 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1008 15:28:30.742652 2386321 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I1008 15:28:30.742690 2386321 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1008 15:28:30.742712 2386321 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I1008 15:28:30.742733 2386321 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I1008 15:28:30.742763 2386321 kubeadm.go:310] [certs] Generating "sa" key and public key
I1008 15:28:30.742794 2386321 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1008 15:28:30.742820 2386321 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I1008 15:28:30.742841 2386321 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I1008 15:28:30.742860 2386321 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1008 15:28:30.742890 2386321 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1008 15:28:30.742909 2386321 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1008 15:28:30.742938 2386321 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1008 15:28:30.742960 2386321 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1008 15:28:30.746799 2386321 out.go:235]     ▪ Booting up control plane ...
I1008 15:28:30.746858 2386321 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1008 15:28:30.746905 2386321 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1008 15:28:30.746944 2386321 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1008 15:28:30.747000 2386321 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1008 15:28:30.747040 2386321 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1008 15:28:30.747059 2386321 kubeadm.go:310] [kubelet-start] Starting the kubelet
I1008 15:28:30.747120 2386321 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I1008 15:28:30.747169 2386321 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I1008 15:28:30.747196 2386321 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 501.01772ms
I1008 15:28:30.747232 2386321 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I1008 15:28:30.747259 2386321 kubeadm.go:310] [api-check] The API server is healthy after 9.50141143s
I1008 15:28:30.747312 2386321 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1008 15:28:30.747376 2386321 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1008 15:28:30.747416 2386321 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I1008 15:28:30.747492 2386321 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1008 15:28:30.747509 2386321 kubeadm.go:310] [bootstrap-token] Using token: 8qabz8.kyuytgb9h82mc6vk
I1008 15:28:30.751199 2386321 out.go:235]     ▪ Configuring RBAC rules ...
I1008 15:28:30.751239 2386321 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1008 15:28:30.751265 2386321 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1008 15:28:30.751316 2386321 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1008 15:28:30.751363 2386321 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1008 15:28:30.751416 2386321 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1008 15:28:30.751447 2386321 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1008 15:28:30.751501 2386321 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1008 15:28:30.751516 2386321 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I1008 15:28:30.751531 2386321 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I1008 15:28:30.751533 2386321 kubeadm.go:310] 
I1008 15:28:30.751551 2386321 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I1008 15:28:30.751553 2386321 kubeadm.go:310] 
I1008 15:28:30.751577 2386321 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I1008 15:28:30.751578 2386321 kubeadm.go:310] 
I1008 15:28:30.751586 2386321 kubeadm.go:310]   mkdir -p $HOME/.kube
I1008 15:28:30.751605 2386321 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1008 15:28:30.751621 2386321 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1008 15:28:30.751622 2386321 kubeadm.go:310] 
I1008 15:28:30.751640 2386321 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I1008 15:28:30.751641 2386321 kubeadm.go:310] 
I1008 15:28:30.751656 2386321 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1008 15:28:30.751657 2386321 kubeadm.go:310] 
I1008 15:28:30.751673 2386321 kubeadm.go:310] You should now deploy a pod network to the cluster.
I1008 15:28:30.751697 2386321 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1008 15:28:30.751719 2386321 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1008 15:28:30.751720 2386321 kubeadm.go:310] 
I1008 15:28:30.751746 2386321 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I1008 15:28:30.751778 2386321 kubeadm.go:310] and service account keys on each node and then running the following as root:
I1008 15:28:30.751779 2386321 kubeadm.go:310] 
I1008 15:28:30.751809 2386321 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token 8qabz8.kyuytgb9h82mc6vk \
I1008 15:28:30.751845 2386321 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:bf54efea6f92e6ebfdf2ba69ef8fa5f266fe622a10a6219c0d7403e49c7474af \
I1008 15:28:30.751852 2386321 kubeadm.go:310] 	--control-plane 
I1008 15:28:30.751853 2386321 kubeadm.go:310] 
I1008 15:28:30.751883 2386321 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I1008 15:28:30.751885 2386321 kubeadm.go:310] 
I1008 15:28:30.751913 2386321 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token 8qabz8.kyuytgb9h82mc6vk \
I1008 15:28:30.751953 2386321 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:bf54efea6f92e6ebfdf2ba69ef8fa5f266fe622a10a6219c0d7403e49c7474af 
I1008 15:28:30.751957 2386321 cni.go:84] Creating CNI manager for ""
I1008 15:28:30.751964 2386321 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1008 15:28:30.755794 2386321 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I1008 15:28:30.763217 2386321 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1008 15:28:30.769280 2386321 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I1008 15:28:30.780704 2386321 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1008 15:28:30.780722 2386321 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.31.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1008 15:28:30.780738 2386321 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_10_08T15_28_30_0700 minikube.k8s.io/version=v1.34.0 minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I1008 15:28:30.785751 2386321 ops.go:34] apiserver oom_adj: -16
I1008 15:28:30.858063 2386321 kubeadm.go:1113] duration metric: took 77.35766ms to wait for elevateKubeSystemPrivileges
I1008 15:28:30.878984 2386321 kubeadm.go:394] duration metric: took 12.746685134s to StartCluster
I1008 15:28:30.878996 2386321 settings.go:142] acquiring lock: {Name:mkdf43793a074d7e0de2cab3f84078cc82187bcc Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1008 15:28:30.879030 2386321 settings.go:150] Updating kubeconfig:  /home/matthew/.kube/config
I1008 15:28:30.879322 2386321 lock.go:35] WriteFile acquiring /home/matthew/.kube/config: {Name:mk7f93ffd29e80fba2d44a9e186cee928b5777bd Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1008 15:28:30.879439 2386321 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1008 15:28:30.879445 2386321 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1008 15:28:30.879481 2386321 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1008 15:28:30.879504 2386321 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1008 15:28:30.879515 2386321 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I1008 15:28:30.879521 2386321 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1008 15:28:30.879528 2386321 host.go:66] Checking if "minikube" exists ...
I1008 15:28:30.879537 2386321 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1008 15:28:30.879565 2386321 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1008 15:28:30.879694 2386321 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1008 15:28:30.879753 2386321 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1008 15:28:30.889798 2386321 addons.go:234] Setting addon default-storageclass=true in "minikube"
I1008 15:28:30.889812 2386321 host.go:66] Checking if "minikube" exists ...
I1008 15:28:30.889981 2386321 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1008 15:28:30.898806 2386321 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I1008 15:28:30.898812 2386321 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1008 15:28:30.898837 2386321 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1008 15:28:30.908191 2386321 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32808 SSHKeyPath:/home/matthew/.minikube/machines/minikube/id_rsa Username:docker}
I1008 15:28:30.916395 2386321 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1008 15:28:30.920516 2386321 out.go:177] 🔎  Verifying Kubernetes components...
I1008 15:28:30.920516 2386321 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1008 15:28:30.924318 2386321 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1008 15:28:30.924349 2386321 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1008 15:28:30.924353 2386321 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1008 15:28:30.924375 2386321 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1008 15:28:30.935125 2386321 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32808 SSHKeyPath:/home/matthew/.minikube/machines/minikube/id_rsa Username:docker}
I1008 15:28:30.993849 2386321 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1008 15:28:31.020333 2386321 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1008 15:28:31.104547 2386321 start.go:971] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I1008 15:28:31.104616 2386321 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1008 15:28:31.439735 2386321 api_server.go:52] waiting for apiserver process to appear ...
I1008 15:28:31.439757 2386321 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1008 15:28:31.447336 2386321 api_server.go:72] duration metric: took 567.877907ms to wait for apiserver process to appear ...
I1008 15:28:31.447345 2386321 api_server.go:88] waiting for apiserver healthz status ...
I1008 15:28:31.447352 2386321 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1008 15:28:31.447412 2386321 out.go:177] 🌟  Enabled addons: default-storageclass, storage-provisioner
I1008 15:28:31.449327 2386321 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I1008 15:28:31.449718 2386321 api_server.go:141] control plane version: v1.31.0
I1008 15:28:31.449724 2386321 api_server.go:131] duration metric: took 2.376732ms to wait for apiserver health ...
I1008 15:28:31.449727 2386321 system_pods.go:43] waiting for kube-system pods to appear ...
I1008 15:28:31.451202 2386321 addons.go:510] duration metric: took 571.722885ms for enable addons: enabled=[default-storageclass storage-provisioner]
I1008 15:28:31.451912 2386321 system_pods.go:59] 5 kube-system pods found
I1008 15:28:31.451919 2386321 system_pods.go:61] "etcd-minikube" [0816d2be-e9aa-46ab-8210-0437aae57f3d] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1008 15:28:31.451922 2386321 system_pods.go:61] "kube-apiserver-minikube" [5120b978-924c-476f-a8c3-2a55aa13bc5f] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1008 15:28:31.451926 2386321 system_pods.go:61] "kube-controller-manager-minikube" [b327b30a-b46b-40cf-8c3b-3f576f71e9f6] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1008 15:28:31.451928 2386321 system_pods.go:61] "kube-scheduler-minikube" [fce0c36a-beb4-471b-a8fa-dd343d2ee342] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1008 15:28:31.451930 2386321 system_pods.go:61] "storage-provisioner" [a1b8ecbf-cc16-44e3-8058-fab638d42b2f] Pending
I1008 15:28:31.451932 2386321 system_pods.go:74] duration metric: took 2.202393ms to wait for pod list to return data ...
I1008 15:28:31.451935 2386321 kubeadm.go:582] duration metric: took 572.479882ms to wait for: map[apiserver:true system_pods:true]
I1008 15:28:31.451941 2386321 node_conditions.go:102] verifying NodePressure condition ...
I1008 15:28:31.452987 2386321 node_conditions.go:122] node storage ephemeral capacity is 15501695516Ki
I1008 15:28:31.452993 2386321 node_conditions.go:123] node cpu capacity is 24
I1008 15:28:31.452998 2386321 node_conditions.go:105] duration metric: took 1.055277ms to run NodePressure ...
I1008 15:28:31.453003 2386321 start.go:241] waiting for startup goroutines ...
I1008 15:28:31.605789 2386321 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1008 15:28:31.605802 2386321 start.go:246] waiting for cluster config update ...
I1008 15:28:31.605806 2386321 start.go:255] writing updated cluster config ...
I1008 15:28:31.605956 2386321 ssh_runner.go:195] Run: rm -f paused
I1008 15:28:31.632847 2386321 start.go:600] kubectl: 1.31.1, cluster: 1.31.0 (minor skew: 0)
I1008 15:28:31.638213 2386321 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Oct 10 14:59:39 minikube cri-dockerd[1551]: time="2024-10-10T14:59:39Z" level=info msg="Stop pulling image bcgsc/pori-graphkb-api:latest: Status: Image is up to date for bcgsc/pori-graphkb-api:latest"
Oct 10 14:59:40 minikube dockerd[1281]: time="2024-10-10T14:59:40.860425958Z" level=info msg="ignoring event" container=e0c247b4d312e11c7ef448f068ea3c04d74906116006f3234e730e92abed666e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:00:11 minikube dockerd[1281]: time="2024-10-10T15:00:11.598585242Z" level=info msg="ignoring event" container=2734fb74e8bc16983f52bb06db1afb787838e379cf2ed13784583351ad77f11e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:00:16 minikube cri-dockerd[1551]: time="2024-10-10T15:00:16Z" level=error msg="error getting RW layer size for container ID 'ceae1d9ca276353d10e9b8e06476562ba8c03519e950a55649b031e5c16e3e4a': Error response from daemon: No such container: ceae1d9ca276353d10e9b8e06476562ba8c03519e950a55649b031e5c16e3e4a"
Oct 10 15:00:16 minikube cri-dockerd[1551]: time="2024-10-10T15:00:16Z" level=error msg="Set backoffDuration to : 1m0s for container ID 'ceae1d9ca276353d10e9b8e06476562ba8c03519e950a55649b031e5c16e3e4a'"
Oct 10 15:00:21 minikube cri-dockerd[1551]: time="2024-10-10T15:00:21Z" level=info msg="Stop pulling image bcgsc/pori-graphkb-api:latest: Status: Image is up to date for bcgsc/pori-graphkb-api:latest"
Oct 10 15:00:22 minikube dockerd[1281]: time="2024-10-10T15:00:22.942733997Z" level=info msg="ignoring event" container=7b0fa4e57d1d6fc38a4806d746e38f34c0aabc3bef4a72d5e894edc99975c81f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:00:27 minikube dockerd[1281]: time="2024-10-10T15:00:27.165379487Z" level=info msg="ignoring event" container=8878d29c7f734f4e148270fd40dd57dd42aea92af0f2568ea9326bf59a56de5b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:00:27 minikube dockerd[1281]: time="2024-10-10T15:00:27.268196880Z" level=info msg="ignoring event" container=67f8e743e03939e34facfa7426b9a9e40b6bb5bcd561cc462a74d0b0c58c86cb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:00:27 minikube dockerd[1281]: time="2024-10-10T15:00:27.432494189Z" level=info msg="ignoring event" container=7f2103ff7a9b998d671df106206147798791068e1ddee3c57a09934c82c85ee5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:00:27 minikube dockerd[1281]: time="2024-10-10T15:00:27.824529465Z" level=info msg="ignoring event" container=8fdd000aaf4a22e21d15458800f67d638b24ec082d064bc2303e08eaa0ca2f59 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:00:32 minikube cri-dockerd[1551]: time="2024-10-10T15:00:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ff6dc2cf6d0f44ca80dcc257d21b7104cc7c5ebb568139119c8339fb899445c8/resolv.conf as [nameserver 10.96.0.10 search graphkb.svc.cluster.local svc.cluster.local cluster.local ucalgary.ca options ndots:5]"
Oct 10 15:00:32 minikube cri-dockerd[1551]: time="2024-10-10T15:00:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/eec47e818cc0b523210cb930ce1a742c4aeccc4be4449768e275873756060ccc/resolv.conf as [nameserver 10.96.0.10 search graphkb.svc.cluster.local svc.cluster.local cluster.local ucalgary.ca options ndots:5]"
Oct 10 15:00:32 minikube cri-dockerd[1551]: time="2024-10-10T15:00:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fd377c1a26718d85892e86db5d4f05ccf8289e44e1e743410f59a688cd1000c6/resolv.conf as [nameserver 10.96.0.10 search graphkb.svc.cluster.local svc.cluster.local cluster.local ucalgary.ca options ndots:5]"
Oct 10 15:00:33 minikube cri-dockerd[1551]: time="2024-10-10T15:00:33Z" level=info msg="Stop pulling image bcgsc/pori-graphkb-api:latest: Status: Image is up to date for bcgsc/pori-graphkb-api:latest"
Oct 10 15:00:34 minikube cri-dockerd[1551]: time="2024-10-10T15:00:34Z" level=info msg="Stop pulling image bcgsc/pori-graphkb-client:latest: Status: Image is up to date for bcgsc/pori-graphkb-client:latest"
Oct 10 15:00:35 minikube dockerd[1281]: time="2024-10-10T15:00:35.033076847Z" level=info msg="ignoring event" container=9ecac2b5a7b6fc5454a79e2ffddff35390ee12b4402ef602d086185f7f45be58 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:00:35 minikube dockerd[1281]: time="2024-10-10T15:00:35.462107373Z" level=info msg="ignoring event" container=8bd8756d28750ab75c9945cf37ed06471453b09f3c5dd77088c5e19335d17d28 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:00:35 minikube cri-dockerd[1551]: time="2024-10-10T15:00:35Z" level=error msg="error getting RW layer size for container ID '8878d29c7f734f4e148270fd40dd57dd42aea92af0f2568ea9326bf59a56de5b': Error response from daemon: No such container: 8878d29c7f734f4e148270fd40dd57dd42aea92af0f2568ea9326bf59a56de5b"
Oct 10 15:00:35 minikube cri-dockerd[1551]: time="2024-10-10T15:00:35Z" level=error msg="Set backoffDuration to : 1m0s for container ID '8878d29c7f734f4e148270fd40dd57dd42aea92af0f2568ea9326bf59a56de5b'"
Oct 10 15:00:37 minikube cri-dockerd[1551]: time="2024-10-10T15:00:37Z" level=info msg="Stop pulling image bcgsc/pori-graphkb-api:latest: Status: Image is up to date for bcgsc/pori-graphkb-api:latest"
Oct 10 15:00:38 minikube dockerd[1281]: time="2024-10-10T15:00:38.448189014Z" level=info msg="ignoring event" container=1c8e451e984c0493a01884894554e9624744c1d2ca74ee5ab1cbc5af1984ad20 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:00:38 minikube dockerd[1281]: time="2024-10-10T15:00:38.941498021Z" level=info msg="ignoring event" container=64c76670f4e52fe5e23232c7dce4338f92bdbb7ccdbc16eb422d4993a3f9d80e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:00:54 minikube cri-dockerd[1551]: time="2024-10-10T15:00:54Z" level=info msg="Stop pulling image bcgsc/pori-graphkb-api:latest: Status: Image is up to date for bcgsc/pori-graphkb-api:latest"
Oct 10 15:00:56 minikube dockerd[1281]: time="2024-10-10T15:00:56.164807400Z" level=info msg="ignoring event" container=4128435536cb7196243aa086833b6b9da80f551ac393246fa86744d85ffef547 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:00:56 minikube dockerd[1281]: time="2024-10-10T15:00:56.839592511Z" level=info msg="ignoring event" container=a9f3c11ba200e3dad3a52c739d07e80d0ccf0f6d780800ed97c1506fa7caa32d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:01:21 minikube dockerd[1281]: time="2024-10-10T15:01:21.631787055Z" level=info msg="ignoring event" container=af15057513af66ffd44c9283101e657f75e9e813594feb1040f834154cf54bd2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:01:28 minikube cri-dockerd[1551]: time="2024-10-10T15:01:28Z" level=info msg="Stop pulling image bcgsc/pori-graphkb-api:latest: Status: Image is up to date for bcgsc/pori-graphkb-api:latest"
Oct 10 15:01:30 minikube dockerd[1281]: time="2024-10-10T15:01:30.003773322Z" level=info msg="ignoring event" container=93cca554688e79418c6c956c52340898d19914c06830a99a86ee1b2c798a8e42 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:02:05 minikube dockerd[1281]: time="2024-10-10T15:02:05.771164981Z" level=info msg="ignoring event" container=5688ad9827b52c44995db87599669af7c2055f07582686a38f360c906ae80dcf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:02:10 minikube dockerd[1281]: time="2024-10-10T15:02:10.002570039Z" level=info msg="ignoring event" container=fd377c1a26718d85892e86db5d4f05ccf8289e44e1e743410f59a688cd1000c6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:02:12 minikube cri-dockerd[1551]: time="2024-10-10T15:02:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/461e9a9bfc3ada1bcc839512bc2cd763b1d8114c9a46c083bcab245553c601a2/resolv.conf as [nameserver 10.96.0.10 search graphkb.svc.cluster.local svc.cluster.local cluster.local ucalgary.ca options ndots:5]"
Oct 10 15:02:14 minikube dockerd[1281]: time="2024-10-10T15:02:14.006565313Z" level=info msg="ignoring event" container=34ce65f64c679aca9e7d329d14b8005fbde172464fda3d84bcb2e5bf78a762ea module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:02:16 minikube dockerd[1281]: time="2024-10-10T15:02:16.396573325Z" level=info msg="ignoring event" container=5b8af4b798077636b04348967ba6c4c8578a58d787803941f316af5457532915 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:02:22 minikube cri-dockerd[1551]: time="2024-10-10T15:02:22Z" level=info msg="Stop pulling image bcgsc/pori-graphkb-api:latest: Status: Image is up to date for bcgsc/pori-graphkb-api:latest"
Oct 10 15:02:23 minikube dockerd[1281]: time="2024-10-10T15:02:23.974956622Z" level=info msg="ignoring event" container=d9dc36c75300f80f741890ee2efb6f5d4792d41d6547b1dcc0f383bf871f0ba0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:02:31 minikube dockerd[1281]: time="2024-10-10T15:02:31.618202603Z" level=info msg="ignoring event" container=8baea935ebec74d4c5723bc4c869f86662209135afa66a7c4e4ffee05b777517 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:02:45 minikube dockerd[1281]: time="2024-10-10T15:02:45.428865634Z" level=info msg="ignoring event" container=461e9a9bfc3ada1bcc839512bc2cd763b1d8114c9a46c083bcab245553c601a2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:02:48 minikube cri-dockerd[1551]: time="2024-10-10T15:02:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f5a1e2263282acaed5c82df44225c72a23d70b94290eeb5c92693e0ed2a05ad4/resolv.conf as [nameserver 10.96.0.10 search graphkb.svc.cluster.local svc.cluster.local cluster.local ucalgary.ca options ndots:5]"
Oct 10 15:02:50 minikube dockerd[1281]: time="2024-10-10T15:02:50.394430554Z" level=info msg="ignoring event" container=191854a1564fb4684ad9d206d4e82f2c96a960630669c307f39b49f903d884eb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:02:52 minikube dockerd[1281]: time="2024-10-10T15:02:52.902041409Z" level=info msg="ignoring event" container=58af568060ada068dbaab53de197f4e884dcb46451bcf94e9d237151cbf66d8f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:03:08 minikube dockerd[1281]: time="2024-10-10T15:03:08.899946310Z" level=info msg="ignoring event" container=1f05804a200102ff926fe451a8d48107eca5594f15a12bb17d65ea90e6e66554 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:03:38 minikube dockerd[1281]: time="2024-10-10T15:03:38.759287386Z" level=info msg="ignoring event" container=03fc5bc590e28ab3c9ba6fcf86754ac1732aefb36d731dcb2b1b571e4c726d1d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:03:49 minikube cri-dockerd[1551]: time="2024-10-10T15:03:49Z" level=info msg="Stop pulling image bcgsc/pori-graphkb-api:latest: Status: Image is up to date for bcgsc/pori-graphkb-api:latest"
Oct 10 15:03:50 minikube dockerd[1281]: time="2024-10-10T15:03:50.905718859Z" level=info msg="ignoring event" container=c174246e37d9025f85c1d434875b554c376948e7de9dc9e6585389bc1bdbe7ab module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:04:30 minikube dockerd[1281]: time="2024-10-10T15:04:30.639372131Z" level=info msg="ignoring event" container=8d7778094603b53c32393db60e9796dee558f23ca1bbd584a86eb51bf7cd8930 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:05:53 minikube dockerd[1281]: time="2024-10-10T15:05:53.539732677Z" level=info msg="ignoring event" container=d0dbf653fc9d5338cc2cd63edea070a7a5cab56ebdd3c6afe38022e866472d0f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:06:39 minikube cri-dockerd[1551]: time="2024-10-10T15:06:39Z" level=info msg="Stop pulling image bcgsc/pori-graphkb-api:latest: Status: Image is up to date for bcgsc/pori-graphkb-api:latest"
Oct 10 15:06:40 minikube dockerd[1281]: time="2024-10-10T15:06:40.893552713Z" level=info msg="ignoring event" container=c03dcc0794875d7aaea9ee5fb2cff4d6ecfceb0b99accc156c2f9bbaf746f9bc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:08:42 minikube dockerd[1281]: time="2024-10-10T15:08:42.626731848Z" level=info msg="ignoring event" container=e7d2ec6f1b6173d62f4225d2221f6c37817bf5403ff31c505c4308d18a862d31 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:11:24 minikube dockerd[1281]: time="2024-10-10T15:11:24.160080646Z" level=info msg="ignoring event" container=f5a1e2263282acaed5c82df44225c72a23d70b94290eeb5c92693e0ed2a05ad4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:11:26 minikube cri-dockerd[1551]: time="2024-10-10T15:11:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/628a70e3c8ca3916da95f066e9a13724f053e6cac9712842eb2a95e8e1636c69/resolv.conf as [nameserver 10.96.0.10 search graphkb.svc.cluster.local svc.cluster.local cluster.local ucalgary.ca options ndots:5]"
Oct 10 15:11:42 minikube cri-dockerd[1551]: time="2024-10-10T15:11:42Z" level=info msg="Stop pulling image bcgsc/pori-graphkb-api:latest: Status: Image is up to date for bcgsc/pori-graphkb-api:latest"
Oct 10 15:12:07 minikube dockerd[1281]: time="2024-10-10T15:12:07.088146215Z" level=info msg="ignoring event" container=0481cdca9968ed02f64a988d977f130c8f190e46dd6d9e7ec46b91c4dfa80f7f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:12:07 minikube dockerd[1281]: time="2024-10-10T15:12:07.401753576Z" level=info msg="ignoring event" container=00479e1072220469e2e309d2e6f7ad2a2e626ccfdc5ffb344caf1b3c31cd84b1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:12:08 minikube dockerd[1281]: time="2024-10-10T15:12:08.009739626Z" level=info msg="ignoring event" container=eec47e818cc0b523210cb930ce1a742c4aeccc4be4449768e275873756060ccc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:12:08 minikube dockerd[1281]: time="2024-10-10T15:12:08.043226078Z" level=info msg="ignoring event" container=ff6dc2cf6d0f44ca80dcc257d21b7104cc7c5ebb568139119c8339fb899445c8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:12:36 minikube dockerd[1281]: time="2024-10-10T15:12:36.807487727Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=6beebe71b6dab8dabb8d8afef8c9647d08bcfdab0a2352542c4343b13d2b8bd7
Oct 10 15:12:36 minikube dockerd[1281]: time="2024-10-10T15:12:36.935736497Z" level=info msg="ignoring event" container=6beebe71b6dab8dabb8d8afef8c9647d08bcfdab0a2352542c4343b13d2b8bd7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 10 15:12:37 minikube dockerd[1281]: time="2024-10-10T15:12:37.303620426Z" level=info msg="ignoring event" container=628a70e3c8ca3916da95f066e9a13724f053e6cac9712842eb2a95e8e1636c69 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE                                                                                                  CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
e4703a9ee0645       686d97f312918                                                                                          42 hours ago        Running             keycloak                    0                   bc642554cb8bb       keycloak-685c7bcf74-xgbjx
8b69f8e1699b5       891a27dc84d2e                                                                                          42 hours ago        Running             ipr-api                     0                   1f155228a903f       api-66bdd48d7b-fbkg8
0b7a4ecbc0fd9       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c   42 hours ago        Running             dashboard-metrics-scraper   0                   a01d04b601db9       dashboard-metrics-scraper-c5db448b4-bwpkn
56726cb95a846       kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93         42 hours ago        Running             kubernetes-dashboard        0                   2baa637c0b019       kubernetes-dashboard-695b96c756-j4722
80b15da1e6e16       redis@sha256:2ba50e1ac3a0ea17b736ce9db2b0a9f6f8b85d4c27d5f5accc6a416d8f42c6d5                          42 hours ago        Running             redis                       0                   03fa4b22d6a0a       redis-7bbbc74bfd-6c5zw
ef7e47008ed85       bcgsc/pori-ipr-demodb@sha256:1804e8eafa2b4d22c91739db31529d0cbfff8c6a0aea1aabe7a06de74cce282c          42 hours ago        Running             ipr-db                      0                   feda86e940d41       db-647c9cb999-zzbx7
9572ccfb73471       bcgsc/pori-ipr-client@sha256:afdfc67a4c782185700dca7d914c88deecfdbb0150c07c98b27c61d39ceb3f0f          42 hours ago        Running             ipr-client                  0                   8cb81ec2adf55       client-5bd7bb987d-mbzh6
8471cd981887c       6e38f40d628db                                                                                          42 hours ago        Running             storage-provisioner         1                   73975fe854974       storage-provisioner
53c9a93932616       6e38f40d628db                                                                                          42 hours ago        Exited              storage-provisioner         0                   73975fe854974       storage-provisioner
e3129c04931d5       ad83b2ca7b09e                                                                                          42 hours ago        Running             kube-proxy                  0                   09b2a6b3d1b3f       kube-proxy-87gbj
61a77a251dde3       cbb01a7bd410d                                                                                          42 hours ago        Running             coredns                     0                   f871bb94249dd       coredns-6f6b679f8f-mmkcb
e1c8bb9d73b42       604f5db92eaa8                                                                                          42 hours ago        Running             kube-apiserver              0                   0e7c34a12c21f       kube-apiserver-minikube
9ab52f3dc9724       045733566833c                                                                                          42 hours ago        Running             kube-controller-manager     0                   cab88b04cfe89       kube-controller-manager-minikube
343013be97da3       2e96e5913fc06                                                                                          42 hours ago        Running             etcd                        0                   0069e92e3ab8c       etcd-minikube
d4d2ecf126312       1766f54c897f0                                                                                          42 hours ago        Running             kube-scheduler              0                   f537f3008eb28       kube-scheduler-minikube


==> coredns [61a77a251dde] <==
[INFO] 10.244.0.5:45935 - 9612 "A IN redis.db.svc.cluster.local.ipr.svc.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.00014024s
[INFO] 10.244.0.5:47506 - 20053 "A IN redis.db.svc.cluster.local.ipr.svc.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.00017981s
[INFO] 10.244.0.5:53108 - 11201 "A IN redis.db.svc.cluster.local.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000044739s
[INFO] 10.244.0.5:33844 - 49831 "A IN redis.db.svc.cluster.local.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.00004673s
[INFO] 10.244.0.5:41433 - 15798 "A IN redis.db.svc.cluster.local.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.00003473s
[INFO] 10.244.0.5:41074 - 59303 "A IN redis.db.svc.cluster.local.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.00004305s
[INFO] 10.244.0.5:48068 - 48131 "A IN redis.db.svc.cluster.local.ucalgary.ca. udp 56 false 512" NXDOMAIN qr,rd,ra 143 0.018154997s
[INFO] 10.244.0.5:50113 - 15000 "A IN redis.db.svc.cluster.local.ucalgary.ca. udp 56 false 512" NXDOMAIN qr,rd,ra 143 0.018094237s
[INFO] 10.244.0.5:33824 - 52469 "A IN redis.db.svc.cluster.local. udp 44 false 512" NOERROR qr,aa,rd 86 0.00004887s
[INFO] 10.244.0.5:38190 - 1883 "A IN redis.db.svc.cluster.local. udp 44 false 512" NOERROR qr,aa,rd 86 0.00004573s
[INFO] 10.244.0.5:59237 - 20829 "A IN redis.db.svc.cluster.local.ipr.svc.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.000022589s
[INFO] 10.244.0.5:46871 - 60990 "A IN redis.db.svc.cluster.local.ipr.svc.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.000035209s
[INFO] 10.244.0.5:39223 - 34211 "A IN redis.db.svc.cluster.local.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.00001678s
[INFO] 10.244.0.5:53576 - 63062 "A IN redis.db.svc.cluster.local.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0000118s
[INFO] 10.244.0.5:52553 - 9453 "A IN redis.db.svc.cluster.local.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.00002427s
[INFO] 10.244.0.5:60548 - 21119 "A IN redis.db.svc.cluster.local.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.00001677s
[INFO] 10.244.0.5:41846 - 15618 "A IN redis.db.svc.cluster.local.ucalgary.ca. udp 56 false 512" NXDOMAIN qr,aa,rd,ra 143 0.00001827s
[INFO] 10.244.0.5:37054 - 32161 "A IN redis.db.svc.cluster.local.ucalgary.ca. udp 56 false 512" NXDOMAIN qr,aa,rd,ra 143 0.00001228s
[INFO] 10.244.0.5:35597 - 31561 "A IN redis.db.svc.cluster.local. udp 44 false 512" NOERROR qr,aa,rd 86 0.00002198s
[INFO] 10.244.0.5:54873 - 55770 "A IN redis.db.svc.cluster.local. udp 44 false 512" NOERROR qr,aa,rd 86 0.00001747s
[INFO] 10.244.0.5:38416 - 52763 "A IN redis.db.svc.cluster.local.ipr.svc.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.0000136s
[INFO] 10.244.0.5:59507 - 28609 "A IN redis.db.svc.cluster.local.ipr.svc.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.00001277s
[INFO] 10.244.0.5:59664 - 45931 "A IN redis.db.svc.cluster.local.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.00001432s
[INFO] 10.244.0.5:46501 - 45758 "A IN redis.db.svc.cluster.local.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.00001252s
[INFO] 10.244.0.5:48347 - 48115 "A IN redis.db.svc.cluster.local.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.00002183s
[INFO] 10.244.0.5:33900 - 11828 "A IN redis.db.svc.cluster.local.ucalgary.ca. udp 56 false 512" NXDOMAIN qr,aa,rd,ra 143 0.00001203s
[INFO] 10.244.0.5:53257 - 13561 "A IN redis.db.svc.cluster.local.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.00001058s
[INFO] 10.244.0.5:40424 - 25014 "A IN redis.db.svc.cluster.local. udp 44 false 512" NOERROR qr,aa,rd 86 0.00001058s
[INFO] 10.244.0.5:44705 - 13369 "A IN redis.db.svc.cluster.local.ucalgary.ca. udp 56 false 512" NXDOMAIN qr,aa,rd,ra 143 0.00002248s
[INFO] 10.244.0.5:35948 - 64572 "A IN redis.db.svc.cluster.local. udp 44 false 512" NOERROR qr,aa,rd 86 0.00001135s
[INFO] 10.244.0.5:45392 - 34860 "A IN redis.db.svc.cluster.local.ipr.svc.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.000101849s
[INFO] 10.244.0.5:47398 - 51873 "A IN redis.db.svc.cluster.local.ipr.svc.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.000153039s
[INFO] 10.244.0.5:51378 - 28456 "A IN redis.db.svc.cluster.local.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.00007217s
[INFO] 10.244.0.5:40187 - 57514 "A IN redis.db.svc.cluster.local.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.00009065s
[INFO] 10.244.0.5:45182 - 3991 "A IN redis.db.svc.cluster.local.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.00002715s
[INFO] 10.244.0.5:52797 - 43267 "A IN redis.db.svc.cluster.local.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.00004468s
[INFO] 10.244.0.5:60443 - 43112 "A IN redis.db.svc.cluster.local.ucalgary.ca. udp 56 false 512" NXDOMAIN qr,rd,ra 143 0.013108565s
[INFO] 10.244.0.5:34967 - 25390 "A IN redis.db.svc.cluster.local.ucalgary.ca. udp 56 false 512" NXDOMAIN qr,rd,ra 143 0.013041725s
[INFO] 10.244.0.5:36601 - 25959 "A IN redis.db.svc.cluster.local. udp 44 false 512" NOERROR qr,aa,rd 86 0.00004308s
[INFO] 10.244.0.5:60890 - 58632 "A IN redis.db.svc.cluster.local. udp 44 false 512" NOERROR qr,aa,rd 86 0.00003231s
[INFO] 10.244.0.5:42981 - 12563 "A IN redis.db.svc.cluster.local.ipr.svc.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.00001532s
[INFO] 10.244.0.5:58531 - 31713 "A IN redis.db.svc.cluster.local.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000047089s
[INFO] 10.244.0.5:55003 - 12871 "A IN redis.db.svc.cluster.local.ipr.svc.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.000055899s
[INFO] 10.244.0.5:45908 - 60336 "A IN redis.db.svc.cluster.local.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.00001571s
[INFO] 10.244.0.5:45215 - 10276 "A IN redis.db.svc.cluster.local.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.00002836s
[INFO] 10.244.0.5:48869 - 39664 "A IN redis.db.svc.cluster.local.ucalgary.ca. udp 56 false 512" NXDOMAIN qr,aa,rd,ra 143 0.0000251s
[INFO] 10.244.0.5:40298 - 11385 "A IN redis.db.svc.cluster.local.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.00001321s
[INFO] 10.244.0.5:42110 - 41712 "A IN redis.db.svc.cluster.local. udp 44 false 512" NOERROR qr,aa,rd 86 0.00002577s
[INFO] 10.244.0.5:57981 - 38044 "A IN redis.db.svc.cluster.local.ucalgary.ca. udp 56 false 512" NXDOMAIN qr,aa,rd,ra 143 0.00003815s
[INFO] 10.244.0.5:41740 - 18749 "A IN redis.db.svc.cluster.local.ipr.svc.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.00002235s
[INFO] 10.244.0.5:52620 - 23831 "A IN redis.db.svc.cluster.local. udp 44 false 512" NOERROR qr,aa,rd 86 0.00001791s
[INFO] 10.244.0.5:49215 - 15391 "A IN redis.db.svc.cluster.local.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.00002928s
[INFO] 10.244.0.5:54472 - 2674 "A IN redis.db.svc.cluster.local.ipr.svc.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.00001253s
[INFO] 10.244.0.5:46567 - 23275 "A IN redis.db.svc.cluster.local.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.00001798s
[INFO] 10.244.0.5:39582 - 31630 "A IN redis.db.svc.cluster.local.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.00001439s
[INFO] 10.244.0.5:48362 - 58309 "A IN redis.db.svc.cluster.local.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.00001546s
[INFO] 10.244.0.5:49130 - 15582 "A IN redis.db.svc.cluster.local.ucalgary.ca. udp 56 false 512" NXDOMAIN qr,aa,rd,ra 143 0.000030189s
[INFO] 10.244.0.5:35168 - 15370 "A IN redis.db.svc.cluster.local.ucalgary.ca. udp 56 false 512" NXDOMAIN qr,aa,rd,ra 143 0.00001287s
[INFO] 10.244.0.5:41319 - 47822 "A IN redis.db.svc.cluster.local. udp 44 false 512" NOERROR qr,aa,rd 86 0.00001688s
[INFO] 10.244.0.5:55028 - 20813 "A IN redis.db.svc.cluster.local. udp 44 false 512" NOERROR qr,aa,rd 86 0.00001521s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_10_08T15_28_30_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 08 Oct 2024 21:28:25 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 10 Oct 2024 15:14:50 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 10 Oct 2024 15:13:10 +0000   Tue, 08 Oct 2024 21:28:25 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 10 Oct 2024 15:13:10 +0000   Tue, 08 Oct 2024 21:28:25 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 10 Oct 2024 15:13:10 +0000   Tue, 08 Oct 2024 21:28:25 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 10 Oct 2024 15:13:10 +0000   Tue, 08 Oct 2024 21:28:25 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                24
  ephemeral-storage:  15501695516Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             65024240Ki
  pods:               110
Allocatable:
  cpu:                24
  ephemeral-storage:  15501695516Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             65024240Ki
  pods:               110
System Info:
  Machine ID:                 149d515dfdc4438c96b041b9d730bcce
  System UUID:                89a80748-b0c6-4e3a-8f3d-dcf34ce4b802
  Boot ID:                    46f062e8-7b59-4690-a493-b17b90542942
  Kernel Version:             6.8.0-45-generic
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (14 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  db                          redis-7bbbc74bfd-6c5zw                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         41h
  ipr                         api-66bdd48d7b-fbkg8                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         41h
  ipr                         client-5bd7bb987d-mbzh6                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         41h
  ipr                         db-647c9cb999-zzbx7                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         41h
  kube-system                 coredns-6f6b679f8f-mmkcb                     100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     41h
  kube-system                 etcd-minikube                                100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         41h
  kube-system                 kube-apiserver-minikube                      250m (1%)     0 (0%)      0 (0%)           0 (0%)         41h
  kube-system                 kube-controller-manager-minikube             200m (0%)     0 (0%)      0 (0%)           0 (0%)         41h
  kube-system                 kube-proxy-87gbj                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         41h
  kube-system                 kube-scheduler-minikube                      100m (0%)     0 (0%)      0 (0%)           0 (0%)         41h
  kube-system                 storage-provisioner                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         41h
  kubernetes-dashboard        dashboard-metrics-scraper-c5db448b4-bwpkn    0 (0%)        0 (0%)      0 (0%)           0 (0%)         41h
  kubernetes-dashboard        kubernetes-dashboard-695b96c756-j4722        0 (0%)        0 (0%)      0 (0%)           0 (0%)         41h
  security                    keycloak-685c7bcf74-xgbjx                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         41h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (3%)   0 (0%)
  memory             170Mi (0%)  170Mi (0%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:              <none>


==> dmesg <==
[  +0.013833] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:6c:2b:59:ca:71:35:08:00 SRC=10.148.16.220 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=15193 PROTO=2 
[  +0.049267] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:00:4e:01:bf:ad:2d:08:00 SRC=10.148.16.155 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=55878 PROTO=2 
[Oct10 14:57] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:01:ec:eb:b8:11:70:80:08:00 SRC=0.0.0.0 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=23822 PROTO=2 
[  +0.099970] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:54:bf:64:68:5e:f6:08:00 SRC=10.148.16.52 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=65174 PROTO=2 
[  +0.330650] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:24:5e:be:5d:0a:9e:08:00 SRC=136.159.164.122 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[  +0.272107] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:b0:7b:25:01:a8:bd:08:00 SRC=10.148.16.234 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=35941 PROTO=2 
[  +0.101349] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:30:d0:42:fa:46:38:08:00 SRC=10.148.16.27 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[  +0.792695] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:18:03:73:e2:d0:e9:08:00 SRC=10.148.16.98 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=40814 PROTO=2 
[  +6.542432] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:08:92:04:dc:8a:eb:08:00 SRC=10.148.16.215 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=13136 PROTO=2 
[Oct10 14:59] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:01:ec:eb:b8:11:70:80:08:00 SRC=0.0.0.0 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=23823 PROTO=2 
[  +0.026524] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:74:86:e2:1f:55:a8:08:00 SRC=10.148.16.174 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=17933 PROTO=2 
[  +0.433735] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:d8:bb:c1:c4:fc:90:08:00 SRC=10.148.16.62 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=24817 PROTO=2 
[  +0.156229] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:54:bf:64:68:5e:f6:08:00 SRC=10.148.16.52 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=65175 PROTO=2 
[  +0.027154] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:24:5e:be:5d:0a:9e:08:00 SRC=136.159.164.122 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[  +2.083405] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:b0:7b:25:01:a8:bd:08:00 SRC=10.148.16.234 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=35948 PROTO=2 
[Oct10 15:00] kauditd_printk_skb: 14 callbacks suppressed
[Oct10 15:01] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:01:ec:eb:b8:11:70:80:08:00 SRC=0.0.0.0 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=23824 PROTO=2 
[  +0.295486] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:40:8d:5c:ce:2e:af:08:00 SRC=10.148.16.200 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=59416 PROTO=2 
[  +0.217196] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:6c:2b:59:ca:71:35:08:00 SRC=10.148.16.220 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=15196 PROTO=2 
[  +0.357963] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:90:09:d0:48:50:35:08:00 SRC=10.148.16.225 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[  +0.151286] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:d0:81:7a:d5:f5:a6:08:00 SRC=10.148.16.218 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=41300 PROTO=2 
[  +0.430903] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:d0:81:7a:d6:83:21:08:00 SRC=10.148.16.58 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=28141 PROTO=2 
[Oct10 15:03] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:b4:96:91:43:49:45:08:00 SRC=10.148.16.171 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=47441 PROTO=2 
[  +0.061681] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:b4:96:91:43:49:45:08:00 SRC=10.148.16.171 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=47442 PROTO=2 
[ +49.312019] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:01:ec:eb:b8:11:70:80:08:00 SRC=0.0.0.0 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=23825 PROTO=2 
[  +0.012945] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:6c:2b:59:ca:71:35:08:00 SRC=10.148.16.220 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=15197 PROTO=2 
[Oct10 15:04] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:90:09:d0:48:50:35:08:00 SRC=10.148.16.225 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[  +0.423224] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:54:bf:64:68:5e:f6:08:00 SRC=10.148.16.52 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=65177 PROTO=2 
[  +9.661530] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:f4:6d:04:0b:0a:0f:08:00 SRC=10.148.16.154 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[Oct10 15:06] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:01:ec:eb:b8:11:70:80:08:00 SRC=0.0.0.0 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=23826 PROTO=2 
[  +0.114535] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:54:bf:64:68:5e:f6:08:00 SRC=10.148.16.52 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=65178 PROTO=2 
[  +0.243078] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:00:3e:e1:c6:8a:93:08:00 SRC=10.148.16.89 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=16478 PROTO=2 
[  +0.064871] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:24:5e:be:5d:0a:9e:08:00 SRC=136.159.164.122 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[  +0.189451] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:18:03:73:e2:d0:e9:08:00 SRC=10.148.16.98 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=40824 PROTO=2 
[  +3.813448] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:04:42:1a:0c:49:65:08:00 SRC=10.148.16.132 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=53192 PROTO=2 
[Oct10 15:08] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:01:ec:eb:b8:11:70:80:08:00 SRC=0.0.0.0 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=23827 PROTO=2 
[  +0.030516] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:3c:cd:36:5f:d4:0a:08:00 SRC=10.148.16.170 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=25361 PROTO=2 
[  +0.286724] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:d0:03:4b:15:37:45:08:00 SRC=10.148.16.87 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=54007 PROTO=2 
[  +0.205335] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:74:86:e2:1f:55:a8:08:00 SRC=10.148.16.174 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=17938 PROTO=2 
[  +0.242308] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:90:09:d0:48:50:35:08:00 SRC=10.148.16.225 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[  +0.058626] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:3c:7d:0a:08:f3:00:08:00 SRC=10.148.16.57 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=63459 PROTO=2 
[Oct10 15:10] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:01:ec:eb:b8:11:70:80:08:00 SRC=0.0.0.0 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=23828 PROTO=2 
[  +0.010707] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:64:00:6a:66:11:51:08:00 SRC=10.148.16.43 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=31141 PROTO=2 
[  +0.639587] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:9e:91:32:eb:ca:8a:08:00 SRC=10.148.16.217 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[  +0.596946] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:e4:50:eb:bb:59:d7:08:00 SRC=10.148.16.94 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=24807 PROTO=2 
[  +0.380077] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:00:3e:e1:c6:8a:93:08:00 SRC=10.148.16.89 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=61590 PROTO=2 
[  +0.984672] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:18:03:73:e2:d0:e9:08:00 SRC=10.148.16.98 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=40827 PROTO=2 
[ +10.680800] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:3c:7d:0a:08:f3:00:08:00 SRC=10.148.16.57 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=25630 PROTO=2 
[Oct10 15:12] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:01:ec:eb:b8:11:70:80:08:00 SRC=0.0.0.0 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=23829 PROTO=2 
[  +0.105561] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:cc:96:e5:0b:7b:56:08:00 SRC=10.148.16.119 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[  +1.061619] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:08:92:04:dc:8a:eb:08:00 SRC=10.148.16.215 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=13143 PROTO=2 
[  +0.387641] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:00:4e:01:bf:ad:2d:08:00 SRC=10.148.16.155 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=55886 PROTO=2 
[  +0.013639] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:30:d0:42:fa:46:38:08:00 SRC=10.148.16.27 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[  +5.158196] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:00:3e:e1:c6:85:86:08:00 SRC=10.148.16.153 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=64559 PROTO=2 
[Oct10 15:14] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:01:ec:eb:b8:11:70:80:08:00 SRC=0.0.0.0 DST=224.0.0.1 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=23830 PROTO=2 
[  +0.086090] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:74:d4:35:1c:e8:28:08:00 SRC=10.148.16.222 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=10300 PROTO=2 
[  +0.014020] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:58:11:22:4d:71:6e:08:00 SRC=10.148.16.38 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=31750 PROTO=2 
[  +0.122354] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:18:66:da:30:b0:0b:08:00 SRC=10.148.16.121 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=22631 PROTO=2 
[  +0.018100] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:24:5e:be:5d:0a:9e:08:00 SRC=136.159.164.122 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0xC0 TTL=1 ID=0 DF PROTO=2 
[  +0.312668] [UFW BLOCK] IN=eno1 OUT= MAC=01:00:5e:00:00:fb:00:4e:01:bf:ad:2d:08:00 SRC=10.148.16.155 DST=224.0.0.251 LEN=32 TOS=0x00 PREC=0x00 TTL=1 ID=55887 PROTO=2 


==> etcd [343013be97da] <==
{"level":"info","ts":"2024-10-10T15:02:45.714605Z","caller":"traceutil/trace.go:171","msg":"trace[1427784545] transaction","detail":"{read_only:false; response_revision:124441; number_of_response:1; }","duration":"167.846767ms","start":"2024-10-10T15:02:45.546748Z","end":"2024-10-10T15:02:45.714594Z","steps":["trace[1427784545] 'process raft request'  (duration: 167.783857ms)"],"step_count":1}
{"level":"info","ts":"2024-10-10T15:02:47.489502Z","caller":"traceutil/trace.go:171","msg":"trace[386742904] transaction","detail":"{read_only:false; response_revision:124460; number_of_response:1; }","duration":"103.010008ms","start":"2024-10-10T15:02:47.386483Z","end":"2024-10-10T15:02:47.489493Z","steps":["trace[386742904] 'process raft request'  (duration: 61.550574ms)","trace[386742904] 'compare'  (duration: 41.339954ms)"],"step_count":2}
{"level":"info","ts":"2024-10-10T15:02:51.129101Z","caller":"traceutil/trace.go:171","msg":"trace[606608150] transaction","detail":"{read_only:false; response_revision:124472; number_of_response:1; }","duration":"135.815232ms","start":"2024-10-10T15:02:50.993276Z","end":"2024-10-10T15:02:51.129091Z","steps":["trace[606608150] 'process raft request'  (duration: 100.351621ms)","trace[606608150] 'compare'  (duration: 35.426572ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-10T15:02:51.436586Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"146.137707ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128032428788239016 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/endpointslices/graphkb/db-5fnzd\" mod_revision:124468 > success:<request_put:<key:\"/registry/endpointslices/graphkb/db-5fnzd\" value_size:1025 >> failure:<request_range:<key:\"/registry/endpointslices/graphkb/db-5fnzd\" > >>","response":"size:18"}
{"level":"info","ts":"2024-10-10T15:02:51.436670Z","caller":"traceutil/trace.go:171","msg":"trace[2003467195] transaction","detail":"{read_only:false; response_revision:124474; number_of_response:1; }","duration":"305.941592ms","start":"2024-10-10T15:02:51.130715Z","end":"2024-10-10T15:02:51.436656Z","steps":["trace[2003467195] 'process raft request'  (duration: 159.700746ms)","trace[2003467195] 'compare'  (duration: 146.106997ms)"],"step_count":2}
{"level":"info","ts":"2024-10-10T15:02:51.436709Z","caller":"traceutil/trace.go:171","msg":"trace[1605949750] transaction","detail":"{read_only:false; response_revision:124475; number_of_response:1; }","duration":"305.885142ms","start":"2024-10-10T15:02:51.130814Z","end":"2024-10-10T15:02:51.436699Z","steps":["trace[1605949750] 'process raft request'  (duration: 305.803192ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-10T15:02:51.436717Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-10T15:02:51.130705Z","time spent":"305.991032ms","remote":"127.0.0.1:46120","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1074,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/endpointslices/graphkb/db-5fnzd\" mod_revision:124468 > success:<request_put:<key:\"/registry/endpointslices/graphkb/db-5fnzd\" value_size:1025 >> failure:<request_range:<key:\"/registry/endpointslices/graphkb/db-5fnzd\" > >"}
{"level":"warn","ts":"2024-10-10T15:02:51.436754Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-10T15:02:51.130810Z","time spent":"305.921642ms","remote":"127.0.0.1:46342","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":3147,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/replicasets/graphkb/db-5945474745\" mod_revision:124469 > success:<request_put:<key:\"/registry/replicasets/graphkb/db-5945474745\" value_size:3096 >> failure:<request_range:<key:\"/registry/replicasets/graphkb/db-5945474745\" > >"}
{"level":"info","ts":"2024-10-10T15:02:51.929929Z","caller":"traceutil/trace.go:171","msg":"trace[377463600] transaction","detail":"{read_only:false; response_revision:124477; number_of_response:1; }","duration":"118.772429ms","start":"2024-10-10T15:02:51.811147Z","end":"2024-10-10T15:02:51.929919Z","steps":["trace[377463600] 'process raft request'  (duration: 118.712479ms)"],"step_count":1}
{"level":"info","ts":"2024-10-10T15:03:09.484993Z","caller":"traceutil/trace.go:171","msg":"trace[776390236] transaction","detail":"{read_only:false; response_revision:124510; number_of_response:1; }","duration":"116.987482ms","start":"2024-10-10T15:03:09.367993Z","end":"2024-10-10T15:03:09.484981Z","steps":["trace[776390236] 'process raft request'  (duration: 23.626455ms)","trace[776390236] 'compare'  (duration: 93.310048ms)"],"step_count":2}
{"level":"info","ts":"2024-10-10T15:03:09.493255Z","caller":"traceutil/trace.go:171","msg":"trace[1105873310] transaction","detail":"{read_only:false; response_revision:124512; number_of_response:1; }","duration":"124.887068ms","start":"2024-10-10T15:03:09.368359Z","end":"2024-10-10T15:03:09.493246Z","steps":["trace[1105873310] 'process raft request'  (duration: 124.864798ms)"],"step_count":1}
{"level":"info","ts":"2024-10-10T15:03:09.493310Z","caller":"traceutil/trace.go:171","msg":"trace[740307795] transaction","detail":"{read_only:false; response_revision:124511; number_of_response:1; }","duration":"125.279156ms","start":"2024-10-10T15:03:09.368025Z","end":"2024-10-10T15:03:09.493304Z","steps":["trace[740307795] 'process raft request'  (duration: 125.158076ms)"],"step_count":1}
{"level":"info","ts":"2024-10-10T15:03:37.371249Z","caller":"traceutil/trace.go:171","msg":"trace[1090056146] transaction","detail":"{read_only:false; response_revision:124539; number_of_response:1; }","duration":"141.768443ms","start":"2024-10-10T15:03:37.229471Z","end":"2024-10-10T15:03:37.371239Z","steps":["trace[1090056146] 'process raft request'  (duration: 116.694933ms)","trace[1090056146] 'compare'  (duration: 25.012641ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-10T15:03:37.622369Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"140.997367ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard\" ","response":"range_response_count:1 size:869"}
{"level":"warn","ts":"2024-10-10T15:03:37.622382Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"141.877043ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard\" ","response":"range_response_count:1 size:869"}
{"level":"info","ts":"2024-10-10T15:03:37.622414Z","caller":"traceutil/trace.go:171","msg":"trace[98409826] range","detail":"{range_begin:/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard; range_end:; response_count:1; response_revision:124539; }","duration":"141.049536ms","start":"2024-10-10T15:03:37.481355Z","end":"2024-10-10T15:03:37.622404Z","steps":["trace[98409826] 'range keys from in-memory index tree'  (duration: 140.929377ms)"],"step_count":1}
{"level":"info","ts":"2024-10-10T15:03:37.622432Z","caller":"traceutil/trace.go:171","msg":"trace[1256369603] range","detail":"{range_begin:/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard; range_end:; response_count:1; response_revision:124539; }","duration":"141.935692ms","start":"2024-10-10T15:03:37.480486Z","end":"2024-10-10T15:03:37.622421Z","steps":["trace[1256369603] 'range keys from in-memory index tree'  (duration: 141.783733ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-10T15:03:37.622437Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"141.769163ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard\" ","response":"range_response_count:1 size:869"}
{"level":"warn","ts":"2024-10-10T15:03:37.622369Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"141.345225ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard\" ","response":"range_response_count:1 size:869"}
{"level":"info","ts":"2024-10-10T15:03:37.622460Z","caller":"traceutil/trace.go:171","msg":"trace[340948826] range","detail":"{range_begin:/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard; range_end:; response_count:1; response_revision:124539; }","duration":"141.792763ms","start":"2024-10-10T15:03:37.480661Z","end":"2024-10-10T15:03:37.622454Z","steps":["trace[340948826] 'range keys from in-memory index tree'  (duration: 141.636494ms)"],"step_count":1}
{"level":"info","ts":"2024-10-10T15:03:37.622473Z","caller":"traceutil/trace.go:171","msg":"trace[1702035603] range","detail":"{range_begin:/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard; range_end:; response_count:1; response_revision:124539; }","duration":"141.460754ms","start":"2024-10-10T15:03:37.481008Z","end":"2024-10-10T15:03:37.622468Z","steps":["trace[1702035603] 'range keys from in-memory index tree'  (duration: 141.285146ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-10T15:03:37.622498Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"141.270975ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard\" ","response":"range_response_count:1 size:869"}
{"level":"info","ts":"2024-10-10T15:03:37.622512Z","caller":"traceutil/trace.go:171","msg":"trace[1060505377] range","detail":"{range_begin:/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard; range_end:; response_count:1; response_revision:124539; }","duration":"141.283795ms","start":"2024-10-10T15:03:37.481222Z","end":"2024-10-10T15:03:37.622506Z","steps":["trace[1060505377] 'range keys from in-memory index tree'  (duration: 141.093726ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-10T15:03:37.622544Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"141.755203ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard\" ","response":"range_response_count:1 size:869"}
{"level":"info","ts":"2024-10-10T15:03:37.622564Z","caller":"traceutil/trace.go:171","msg":"trace[229400185] range","detail":"{range_begin:/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard; range_end:; response_count:1; response_revision:124539; }","duration":"141.776323ms","start":"2024-10-10T15:03:37.480781Z","end":"2024-10-10T15:03:37.622557Z","steps":["trace[229400185] 'range keys from in-memory index tree'  (duration: 141.518114ms)"],"step_count":1}
{"level":"info","ts":"2024-10-10T15:03:39.620766Z","caller":"traceutil/trace.go:171","msg":"trace[163212324] transaction","detail":"{read_only:false; response_revision:124550; number_of_response:1; }","duration":"142.676069ms","start":"2024-10-10T15:03:39.478079Z","end":"2024-10-10T15:03:39.620755Z","steps":["trace[163212324] 'process raft request'  (duration: 69.925216ms)","trace[163212324] 'compare'  (duration: 72.707894ms)"],"step_count":2}
{"level":"info","ts":"2024-10-10T15:03:39.636115Z","caller":"traceutil/trace.go:171","msg":"trace[1757049315] transaction","detail":"{read_only:false; response_revision:124551; number_of_response:1; }","duration":"158.030613ms","start":"2024-10-10T15:03:39.478079Z","end":"2024-10-10T15:03:39.636109Z","steps":["trace[1757049315] 'process raft request'  (duration: 157.981113ms)"],"step_count":1}
{"level":"info","ts":"2024-10-10T15:03:39.636161Z","caller":"traceutil/trace.go:171","msg":"trace[719553621] transaction","detail":"{read_only:false; response_revision:124552; number_of_response:1; }","duration":"157.875173ms","start":"2024-10-10T15:03:39.478283Z","end":"2024-10-10T15:03:39.636158Z","steps":["trace[719553621] 'process raft request'  (duration: 157.807994ms)"],"step_count":1}
{"level":"info","ts":"2024-10-10T15:03:49.500417Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":123896}
{"level":"info","ts":"2024-10-10T15:03:49.592010Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":123896,"took":"91.283176ms","hash":286106669,"current-db-size-bytes":4837376,"current-db-size":"4.8 MB","current-db-size-in-use-bytes":3346432,"current-db-size-in-use":"3.3 MB"}
{"level":"info","ts":"2024-10-10T15:03:49.592032Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":286106669,"revision":123896,"compact-revision":123131}
{"level":"info","ts":"2024-10-10T15:05:53.409257Z","caller":"traceutil/trace.go:171","msg":"trace[92569853] transaction","detail":"{read_only:false; response_revision:124675; number_of_response:1; }","duration":"131.767247ms","start":"2024-10-10T15:05:53.277482Z","end":"2024-10-10T15:05:53.409250Z","steps":["trace[92569853] 'process raft request'  (duration: 64.927058ms)","trace[92569853] 'compare'  (duration: 66.785869ms)"],"step_count":2}
{"level":"info","ts":"2024-10-10T15:05:53.409314Z","caller":"traceutil/trace.go:171","msg":"trace[1815551367] transaction","detail":"{read_only:false; response_revision:124676; number_of_response:1; }","duration":"131.468137ms","start":"2024-10-10T15:05:53.277843Z","end":"2024-10-10T15:05:53.409311Z","steps":["trace[1815551367] 'process raft request'  (duration: 131.390098ms)"],"step_count":1}
{"level":"info","ts":"2024-10-10T15:05:54.422059Z","caller":"traceutil/trace.go:171","msg":"trace[1396278068] transaction","detail":"{read_only:false; response_revision:124681; number_of_response:1; }","duration":"107.998392ms","start":"2024-10-10T15:05:54.314052Z","end":"2024-10-10T15:05:54.422050Z","steps":["trace[1396278068] 'process raft request'  (duration: 76.952868ms)","trace[1396278068] 'compare'  (duration: 30.981295ms)"],"step_count":2}
{"level":"info","ts":"2024-10-10T15:05:54.422127Z","caller":"traceutil/trace.go:171","msg":"trace[2117370625] transaction","detail":"{read_only:false; response_revision:124682; number_of_response:1; }","duration":"107.857263ms","start":"2024-10-10T15:05:54.314265Z","end":"2024-10-10T15:05:54.422122Z","steps":["trace[2117370625] 'process raft request'  (duration: 107.751133ms)"],"step_count":1}
{"level":"info","ts":"2024-10-10T15:08:43.219582Z","caller":"traceutil/trace.go:171","msg":"trace[1757257733] transaction","detail":"{read_only:false; response_revision:124828; number_of_response:1; }","duration":"127.44169ms","start":"2024-10-10T15:08:43.092130Z","end":"2024-10-10T15:08:43.219572Z","steps":["trace[1757257733] 'process raft request'  (duration: 90.30177ms)","trace[1757257733] 'compare'  (duration: 37.09289ms)"],"step_count":2}
{"level":"info","ts":"2024-10-10T15:08:43.219634Z","caller":"traceutil/trace.go:171","msg":"trace[1271089928] transaction","detail":"{read_only:false; response_revision:124829; number_of_response:1; }","duration":"127.232501ms","start":"2024-10-10T15:08:43.092390Z","end":"2024-10-10T15:08:43.219623Z","steps":["trace[1271089928] 'process raft request'  (duration: 127.161831ms)"],"step_count":1}
{"level":"info","ts":"2024-10-10T15:08:49.550665Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":124562}
{"level":"info","ts":"2024-10-10T15:08:49.633905Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":124562,"took":"82.916318ms","hash":1560815656,"current-db-size-bytes":4837376,"current-db-size":"4.8 MB","current-db-size-in-use-bytes":1925120,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2024-10-10T15:08:49.633927Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1560815656,"revision":124562,"compact-revision":123896}
{"level":"info","ts":"2024-10-10T15:12:06.802543Z","caller":"traceutil/trace.go:171","msg":"trace[889232] transaction","detail":"{read_only:false; response_revision:125048; number_of_response:1; }","duration":"149.486412ms","start":"2024-10-10T15:12:06.653048Z","end":"2024-10-10T15:12:06.802534Z","steps":["trace[889232] 'process raft request'  (duration: 149.433882ms)"],"step_count":1}
{"level":"info","ts":"2024-10-10T15:12:06.802606Z","caller":"traceutil/trace.go:171","msg":"trace[1301729754] transaction","detail":"{read_only:false; response_revision:125049; number_of_response:1; }","duration":"149.497342ms","start":"2024-10-10T15:12:06.653103Z","end":"2024-10-10T15:12:06.802600Z","steps":["trace[1301729754] 'process raft request'  (duration: 149.414022ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-10T15:12:06.802669Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"149.522011ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/graphkb/db-6894d5cc9f-rtjng\" ","response":"range_response_count:1 size:4042"}
{"level":"info","ts":"2024-10-10T15:12:06.802694Z","caller":"traceutil/trace.go:171","msg":"trace[1794913748] range","detail":"{range_begin:/registry/pods/graphkb/db-6894d5cc9f-rtjng; range_end:; response_count:1; response_revision:125049; }","duration":"149.550891ms","start":"2024-10-10T15:12:06.653136Z","end":"2024-10-10T15:12:06.802687Z","steps":["trace[1794913748] 'agreement among raft nodes before linearized reading'  (duration: 149.508161ms)"],"step_count":1}
{"level":"info","ts":"2024-10-10T15:12:06.911354Z","caller":"traceutil/trace.go:171","msg":"trace[1559049533] linearizableReadLoop","detail":"{readStateIndex:156632; appliedIndex:156628; }","duration":"107.651009ms","start":"2024-10-10T15:12:06.803694Z","end":"2024-10-10T15:12:06.911345Z","steps":["trace[1559049533] 'read index received'  (duration: 74.207397ms)","trace[1559049533] 'applied index is now lower than readState.Index'  (duration: 33.443222ms)"],"step_count":2}
{"level":"info","ts":"2024-10-10T15:12:06.911364Z","caller":"traceutil/trace.go:171","msg":"trace[1305505287] transaction","detail":"{read_only:false; response_revision:125050; number_of_response:1; }","duration":"175.658879ms","start":"2024-10-10T15:12:06.735697Z","end":"2024-10-10T15:12:06.911356Z","steps":["trace[1305505287] 'process raft request'  (duration: 142.191807ms)","trace[1305505287] 'compare'  (duration: 33.335622ms)"],"step_count":2}
{"level":"info","ts":"2024-10-10T15:12:06.911381Z","caller":"traceutil/trace.go:171","msg":"trace[2036355695] transaction","detail":"{read_only:false; response_revision:125052; number_of_response:1; }","duration":"174.956531ms","start":"2024-10-10T15:12:06.736417Z","end":"2024-10-10T15:12:06.911373Z","steps":["trace[2036355695] 'process raft request'  (duration: 174.897581ms)"],"step_count":1}
{"level":"info","ts":"2024-10-10T15:12:06.911373Z","caller":"traceutil/trace.go:171","msg":"trace[1724744704] transaction","detail":"{read_only:false; response_revision:125051; number_of_response:1; }","duration":"174.988141ms","start":"2024-10-10T15:12:06.736369Z","end":"2024-10-10T15:12:06.911357Z","steps":["trace[1724744704] 'process raft request'  (duration: 174.926881ms)"],"step_count":1}
{"level":"info","ts":"2024-10-10T15:12:06.911418Z","caller":"traceutil/trace.go:171","msg":"trace[124950911] transaction","detail":"{read_only:false; response_revision:125053; number_of_response:1; }","duration":"108.501837ms","start":"2024-10-10T15:12:06.802900Z","end":"2024-10-10T15:12:06.911402Z","steps":["trace[124950911] 'process raft request'  (duration: 108.432347ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-10T15:12:06.911491Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"107.785038ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/graphkb/api-547b67db8b-h6mkw\" ","response":"range_response_count:1 size:4675"}
{"level":"info","ts":"2024-10-10T15:12:06.911520Z","caller":"traceutil/trace.go:171","msg":"trace[585528967] range","detail":"{range_begin:/registry/pods/graphkb/api-547b67db8b-h6mkw; range_end:; response_count:1; response_revision:125053; }","duration":"107.819168ms","start":"2024-10-10T15:12:06.803692Z","end":"2024-10-10T15:12:06.911511Z","steps":["trace[585528967] 'agreement among raft nodes before linearized reading'  (duration: 107.715419ms)"],"step_count":1}
{"level":"info","ts":"2024-10-10T15:12:07.496604Z","caller":"traceutil/trace.go:171","msg":"trace[1934525306] linearizableReadLoop","detail":"{readStateIndex:156637; appliedIndex:156636; }","duration":"134.714403ms","start":"2024-10-10T15:12:07.361878Z","end":"2024-10-10T15:12:07.496593Z","steps":["trace[1934525306] 'read index received'  (duration: 51.544168ms)","trace[1934525306] 'applied index is now lower than readState.Index'  (duration: 83.169495ms)"],"step_count":2}
{"level":"info","ts":"2024-10-10T15:12:07.496608Z","caller":"traceutil/trace.go:171","msg":"trace[13598862] transaction","detail":"{read_only:false; response_revision:125057; number_of_response:1; }","duration":"141.751028ms","start":"2024-10-10T15:12:07.354844Z","end":"2024-10-10T15:12:07.496595Z","steps":["trace[13598862] 'process raft request'  (duration: 58.586883ms)","trace[13598862] 'compare'  (duration: 83.104346ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-10T15:12:07.496673Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"134.785953ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2024-10-10T15:12:07.496682Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"120.977512ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-10-10T15:12:07.496690Z","caller":"traceutil/trace.go:171","msg":"trace[1720543760] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:125057; }","duration":"134.811163ms","start":"2024-10-10T15:12:07.361875Z","end":"2024-10-10T15:12:07.496686Z","steps":["trace[1720543760] 'agreement among raft nodes before linearized reading'  (duration: 134.758943ms)"],"step_count":1}
{"level":"info","ts":"2024-10-10T15:12:07.496699Z","caller":"traceutil/trace.go:171","msg":"trace[227330302] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:125057; }","duration":"121.001332ms","start":"2024-10-10T15:12:07.375692Z","end":"2024-10-10T15:12:07.496693Z","steps":["trace[227330302] 'agreement among raft nodes before linearized reading'  (duration: 120.966602ms)"],"step_count":1}
{"level":"info","ts":"2024-10-10T15:13:49.609374Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":124837}
{"level":"info","ts":"2024-10-10T15:13:49.642639Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":124837,"took":"33.001226ms","hash":2193416420,"current-db-size-bytes":4837376,"current-db-size":"4.8 MB","current-db-size-in-use-bytes":2019328,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-10-10T15:13:49.642653Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2193416420,"revision":124837,"compact-revision":124562}


==> kernel <==
 15:14:51 up 3 days, 20 min,  0 users,  load average: 0.45, 0.82, 0.89
Linux minikube 6.8.0-45-generic #45-Ubuntu SMP PREEMPT_DYNAMIC Fri Aug 30 12:02:04 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [e1c8bb9d73b4] <==
I1008 21:28:25.299653       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I1008 21:28:25.299844       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1008 21:28:25.299714       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1008 21:28:25.299858       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1008 21:28:25.299949       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1008 21:28:25.299961       1 controller.go:142] Starting OpenAPI controller
I1008 21:28:25.299967       1 naming_controller.go:294] Starting NamingConditionController
I1008 21:28:25.299973       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1008 21:28:25.299977       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1008 21:28:25.299968       1 establishing_controller.go:81] Starting EstablishingController
I1008 21:28:25.299981       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1008 21:28:25.299990       1 crd_finalizer.go:269] Starting CRDFinalizer
I1008 21:28:25.299977       1 controller.go:90] Starting OpenAPI V3 controller
I1008 21:28:25.314784       1 shared_informer.go:320] Caches are synced for node_authorizer
I1008 21:28:25.317896       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1008 21:28:25.317903       1 policy_source.go:224] refreshing policies
E1008 21:28:25.357193       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I1008 21:28:25.399852       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1008 21:28:25.399870       1 aggregator.go:171] initial CRD sync complete...
I1008 21:28:25.399874       1 autoregister_controller.go:144] Starting autoregister controller
I1008 21:28:25.399877       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1008 21:28:25.399879       1 cache.go:39] Caches are synced for autoregister controller
I1008 21:28:25.399883       1 cache.go:39] Caches are synced for LocalAvailability controller
I1008 21:28:25.399962       1 shared_informer.go:320] Caches are synced for configmaps
I1008 21:28:25.400000       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1008 21:28:25.400017       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1008 21:28:25.400030       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1008 21:28:25.400031       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1008 21:28:25.400135       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1008 21:28:25.400141       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1008 21:28:25.400555       1 controller.go:615] quota admission added evaluator for: namespaces
I1008 21:28:25.602042       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1008 21:28:26.310618       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1008 21:28:26.343990       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1008 21:28:26.343999       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1008 21:28:28.636728       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1008 21:28:28.773238       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1008 21:28:28.918486       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W1008 21:28:28.955978       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1008 21:28:28.956493       1 controller.go:615] quota admission added evaluator for: endpoints
I1008 21:28:28.969714       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1008 21:28:29.306354       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I1008 21:28:30.195614       1 controller.go:615] quota admission added evaluator for: deployments.apps
I1008 21:28:30.234922       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I1008 21:28:30.258215       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I1008 21:28:34.057559       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I1008 21:28:34.807666       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I1008 21:32:10.173047       1 alloc.go:330] "allocated clusterIPs" service="graphkb/api" clusterIPs={"IPv4":"10.102.207.155"}
I1008 21:32:10.641824       1 alloc.go:330] "allocated clusterIPs" service="graphkb/client" clusterIPs={"IPv4":"10.108.201.156"}
I1008 21:32:10.985046       1 alloc.go:330] "allocated clusterIPs" service="graphkb/db" clusterIPs={"IPv4":"10.110.192.152"}
I1008 21:32:12.433134       1 alloc.go:330] "allocated clusterIPs" service="ipr/api" clusterIPs={"IPv4":"10.106.220.20"}
I1008 21:32:12.885816       1 alloc.go:330] "allocated clusterIPs" service="ipr/client" clusterIPs={"IPv4":"10.98.39.19"}
I1008 21:32:13.435957       1 alloc.go:330] "allocated clusterIPs" service="ipr/db" clusterIPs={"IPv4":"10.104.116.34"}
I1008 21:32:14.086390       1 alloc.go:330] "allocated clusterIPs" service="security/keycloak" clusterIPs={"IPv4":"10.100.137.99"}
I1008 21:32:14.380251       1 controller.go:615] quota admission added evaluator for: ingresses.networking.k8s.io
I1008 21:32:17.002680       1 alloc.go:330] "allocated clusterIPs" service="db/redis" clusterIPs={"IPv4":"10.97.114.130"}
I1008 21:33:17.326309       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/kubernetes-dashboard" clusterIPs={"IPv4":"10.110.232.21"}
I1008 21:33:17.867972       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/dashboard-metrics-scraper" clusterIPs={"IPv4":"10.103.209.136"}
E1010 15:01:16.826397       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:53284: use of closed network connection
E1010 15:01:32.141853       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:48966: use of closed network connection


==> kube-controller-manager [9ab52f3dc972] <==
I1010 15:02:47.096398       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-644cdb9bb4" duration="26.03µs"
I1010 15:02:47.339996       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="167.714008ms"
I1010 15:02:47.385183       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="45.168388ms"
I1010 15:02:47.385218       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="17.999µs"
I1010 15:02:47.415039       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="17.66µs"
I1010 15:02:50.041993       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="33.254401ms"
I1010 15:02:50.042033       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="22.26µs"
I1010 15:02:51.437178       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="307.427336ms"
I1010 15:02:51.437210       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="17.35µs"
I1010 15:02:53.234088       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="27.13µs"
I1010 15:02:54.292987       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="34.74µs"
I1010 15:02:56.909971       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1010 15:03:08.356703       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="41.600216ms"
I1010 15:03:08.356743       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="22.49µs"
I1010 15:03:09.493715       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="126.63289ms"
I1010 15:03:09.493743       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="16.25µs"
I1010 15:03:21.974047       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="29.089µs"
I1010 15:03:38.466808       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="33.189286ms"
I1010 15:03:38.466847       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="21.95µs"
I1010 15:03:39.636539       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="159.302788ms"
I1010 15:03:39.636571       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="18.58µs"
I1010 15:03:51.606246       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/api-547b67db8b" duration="31.47µs"
I1010 15:03:52.965253       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="27.92µs"
I1010 15:04:00.396960       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/api-547b67db8b" duration="32.69µs"
I1010 15:04:30.940094       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="36.33µs"
I1010 15:04:31.982326       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="31.97µs"
I1010 15:05:53.409786       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="133.454849ms"
I1010 15:05:53.409851       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="42.31µs"
I1010 15:05:54.422513       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="109.263617ms"
I1010 15:05:54.422557       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="28µs"
I1010 15:06:06.961078       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="31.95µs"
I1010 15:06:41.576974       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/api-547b67db8b" duration="42.62µs"
I1010 15:06:56.957343       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/api-547b67db8b" duration="40.139µs"
I1010 15:08:03.033673       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1010 15:08:42.080893       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="41.424604ms"
I1010 15:08:42.080939       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="25.18µs"
I1010 15:08:43.220059       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="128.983694ms"
I1010 15:08:43.220096       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="19.12µs"
I1010 15:08:57.962979       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="31.06µs"
I1010 15:11:23.981794       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="77.835612ms"
I1010 15:11:24.081935       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="100.119371ms"
I1010 15:11:24.081977       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="23.38µs"
I1010 15:11:24.550419       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="78.51µs"
I1010 15:11:25.009997       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="25.08µs"
I1010 15:11:25.026489       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="15.18µs"
I1010 15:11:25.176845       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-6894d5cc9f" duration="89.870639ms"
I1010 15:11:25.251694       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-6894d5cc9f" duration="74.831342ms"
I1010 15:11:25.251727       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-6894d5cc9f" duration="19.77µs"
I1010 15:11:25.268614       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-6894d5cc9f" duration="16.17µs"
I1010 15:11:27.803686       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-6894d5cc9f" duration="67.24958ms"
I1010 15:11:27.803721       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-6894d5cc9f" duration="18.95µs"
I1010 15:11:43.870198       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/api-547b67db8b" duration="75.047333ms"
I1010 15:11:43.870230       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/api-547b67db8b" duration="17.78µs"
I1010 15:12:06.483830       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-644cdb9bb4" duration="2.85µs"
I1010 15:12:06.483977       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-5945474745" duration="3µs"
I1010 15:12:06.483980       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-57f78988f9" duration="3.82µs"
I1010 15:12:06.484019       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/db-6894d5cc9f" duration="2.12µs"
I1010 15:12:06.576816       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/api-547b67db8b" duration="7.069µs"
I1010 15:12:06.600446       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="graphkb/client-c5c645f9d" duration="5.69µs"
I1010 15:13:10.347600       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [e3129c04931d] <==
I1008 21:28:39.772952       1 server_linux.go:66] "Using iptables proxy"
I1008 21:28:39.848255       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1008 21:28:39.848286       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1008 21:28:39.859887       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1008 21:28:39.859908       1 server_linux.go:169] "Using iptables Proxier"
I1008 21:28:39.861281       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1008 21:28:39.861478       1 server.go:483] "Version info" version="v1.31.0"
I1008 21:28:39.861488       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1008 21:28:39.862047       1 config.go:104] "Starting endpoint slice config controller"
I1008 21:28:39.862058       1 config.go:197] "Starting service config controller"
I1008 21:28:39.862071       1 config.go:326] "Starting node config controller"
I1008 21:28:39.862083       1 shared_informer.go:313] Waiting for caches to sync for service config
I1008 21:28:39.862083       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1008 21:28:39.862086       1 shared_informer.go:313] Waiting for caches to sync for node config
I1008 21:28:39.962632       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1008 21:28:39.962663       1 shared_informer.go:320] Caches are synced for service config
I1008 21:28:39.962713       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [d4d2ecf12631] <==
E1008 21:28:23.668186       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get \"https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1008 21:28:23.668047       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1008 21:28:23.668218       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get \"https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1008 21:28:23.668516       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1008 21:28:23.668539       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get \"https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1008 21:28:25.302514       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1008 21:28:25.302622       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1008 21:28:25.302678       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1008 21:28:25.302685       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1008 21:28:25.303205       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1008 21:28:25.303292       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1008 21:28:25.303344       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1008 21:28:25.303359       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1008 21:28:25.303366       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E1008 21:28:25.303355       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1008 21:28:25.303445       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1008 21:28:25.303453       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1008 21:28:25.303480       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1008 21:28:25.303493       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1008 21:28:25.303505       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E1008 21:28:25.303510       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1008 21:28:25.303480       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1008 21:28:25.303526       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1008 21:28:25.303542       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1008 21:28:25.303551       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1008 21:28:25.303730       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1008 21:28:25.303743       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1008 21:28:25.303844       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1008 21:28:25.303853       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1008 21:28:25.303887       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1008 21:28:25.303899       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1008 21:28:25.304075       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1008 21:28:25.304086       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1008 21:28:25.304161       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1008 21:28:25.304173       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W1008 21:28:27.024041       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1008 21:28:27.024067       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1008 21:28:27.096899       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1008 21:28:27.096921       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1008 21:28:27.117492       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1008 21:28:27.117515       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1008 21:28:27.253488       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1008 21:28:27.253507       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1008 21:28:27.258082       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1008 21:28:27.258098       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1008 21:28:27.310820       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1008 21:28:27.310840       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1008 21:28:27.384667       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1008 21:28:27.384683       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1008 21:28:27.427246       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1008 21:28:27.427260       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1008 21:28:27.579188       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1008 21:28:27.579203       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1008 21:28:27.971817       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1008 21:28:27.971840       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1008 21:28:27.977413       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1008 21:28:27.977423       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1008 21:28:27.998026       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1008 21:28:27.998035       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
I1008 21:28:34.268080       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Oct 10 15:13:58 minikube kubelet[2509]: I1010 15:13:58.151578    2509 helpers.go:943] "Eviction manager: no observation found for eviction signal" signal="containerfs.inodesFree"
Oct 10 15:13:58 minikube kubelet[2509]: I1010 15:13:58.151592    2509 helpers.go:943] "Eviction manager: no observation found for eviction signal" signal="containerfs.inodesFree"
Oct 10 15:14:01 minikube kubelet[2509]: E1010 15:14:01.762182    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8b69f8e1699b5ec0a60940f20f6786ec4ca6f72316bb8571d80285ebfbc48668"
Oct 10 15:14:01 minikube kubelet[2509]: E1010 15:14:01.762219    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/ipr_api-66bdd48d7b-fbkg8_31e548dd-a516-41a7-bb34-5bc8161ea278/ipr-api/0.log\": failed to reopen container log \"8b69f8e1699b5ec0a60940f20f6786ec4ca6f72316bb8571d80285ebfbc48668\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8b69f8e1699b5ec0a60940f20f6786ec4ca6f72316bb8571d80285ebfbc48668" path="/var/log/pods/ipr_api-66bdd48d7b-fbkg8_31e548dd-a516-41a7-bb34-5bc8161ea278/ipr-api/0.log" currentSize=35551193 maxSize=10485760
Oct 10 15:14:01 minikube kubelet[2509]: E1010 15:14:01.762973    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="0b7a4ecbc0fd900d4916217b80566c046490e9117460ce356facba654c7c828c"
Oct 10 15:14:01 minikube kubelet[2509]: E1010 15:14:01.763006    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kubernetes-dashboard_dashboard-metrics-scraper-c5db448b4-bwpkn_9c4dbafb-b0e2-4d96-b8c2-e805d495426d/dashboard-metrics-scraper/0.log\": failed to reopen container log \"0b7a4ecbc0fd900d4916217b80566c046490e9117460ce356facba654c7c828c\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="0b7a4ecbc0fd900d4916217b80566c046490e9117460ce356facba654c7c828c" path="/var/log/pods/kubernetes-dashboard_dashboard-metrics-scraper-c5db448b4-bwpkn_9c4dbafb-b0e2-4d96-b8c2-e805d495426d/dashboard-metrics-scraper/0.log" currentSize=15324989 maxSize=10485760
Oct 10 15:14:01 minikube kubelet[2509]: E1010 15:14:01.763664    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="56726cb95a84637cd552c692961dea3fee98c6e2fa86318332c9c6fa6be652d1"
Oct 10 15:14:01 minikube kubelet[2509]: E1010 15:14:01.763689    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kubernetes-dashboard_kubernetes-dashboard-695b96c756-j4722_aebcc8a3-c744-4b8e-97c2-8961f665911b/kubernetes-dashboard/0.log\": failed to reopen container log \"56726cb95a84637cd552c692961dea3fee98c6e2fa86318332c9c6fa6be652d1\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="56726cb95a84637cd552c692961dea3fee98c6e2fa86318332c9c6fa6be652d1" path="/var/log/pods/kubernetes-dashboard_kubernetes-dashboard-695b96c756-j4722_aebcc8a3-c744-4b8e-97c2-8961f665911b/kubernetes-dashboard/0.log" currentSize=30213549 maxSize=10485760
Oct 10 15:14:01 minikube kubelet[2509]: E1010 15:14:01.767620    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="61a77a251dde39a26bc741a87e9f11e44eea79d35296163ac6c431221a177c59"
Oct 10 15:14:01 minikube kubelet[2509]: E1010 15:14:01.767649    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-6f6b679f8f-mmkcb_d99b555e-eaf8-441f-8098-c996824e5048/coredns/0.log\": failed to reopen container log \"61a77a251dde39a26bc741a87e9f11e44eea79d35296163ac6c431221a177c59\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="61a77a251dde39a26bc741a87e9f11e44eea79d35296163ac6c431221a177c59" path="/var/log/pods/kube-system_coredns-6f6b679f8f-mmkcb_d99b555e-eaf8-441f-8098-c996824e5048/coredns/0.log" currentSize=40472222 maxSize=10485760
Oct 10 15:14:08 minikube kubelet[2509]: I1010 15:14:08.164249    2509 helpers.go:943] "Eviction manager: no observation found for eviction signal" signal="containerfs.inodesFree"
Oct 10 15:14:08 minikube kubelet[2509]: I1010 15:14:08.164262    2509 helpers.go:943] "Eviction manager: no observation found for eviction signal" signal="containerfs.inodesFree"
Oct 10 15:14:11 minikube kubelet[2509]: E1010 15:14:11.764269    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8b69f8e1699b5ec0a60940f20f6786ec4ca6f72316bb8571d80285ebfbc48668"
Oct 10 15:14:11 minikube kubelet[2509]: E1010 15:14:11.764311    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/ipr_api-66bdd48d7b-fbkg8_31e548dd-a516-41a7-bb34-5bc8161ea278/ipr-api/0.log\": failed to reopen container log \"8b69f8e1699b5ec0a60940f20f6786ec4ca6f72316bb8571d80285ebfbc48668\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8b69f8e1699b5ec0a60940f20f6786ec4ca6f72316bb8571d80285ebfbc48668" path="/var/log/pods/ipr_api-66bdd48d7b-fbkg8_31e548dd-a516-41a7-bb34-5bc8161ea278/ipr-api/0.log" currentSize=35558327 maxSize=10485760
Oct 10 15:14:11 minikube kubelet[2509]: E1010 15:14:11.764961    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="0b7a4ecbc0fd900d4916217b80566c046490e9117460ce356facba654c7c828c"
Oct 10 15:14:11 minikube kubelet[2509]: E1010 15:14:11.764987    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kubernetes-dashboard_dashboard-metrics-scraper-c5db448b4-bwpkn_9c4dbafb-b0e2-4d96-b8c2-e805d495426d/dashboard-metrics-scraper/0.log\": failed to reopen container log \"0b7a4ecbc0fd900d4916217b80566c046490e9117460ce356facba654c7c828c\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="0b7a4ecbc0fd900d4916217b80566c046490e9117460ce356facba654c7c828c" path="/var/log/pods/kubernetes-dashboard_dashboard-metrics-scraper-c5db448b4-bwpkn_9c4dbafb-b0e2-4d96-b8c2-e805d495426d/dashboard-metrics-scraper/0.log" currentSize=15349031 maxSize=10485760
Oct 10 15:14:11 minikube kubelet[2509]: E1010 15:14:11.765560    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="56726cb95a84637cd552c692961dea3fee98c6e2fa86318332c9c6fa6be652d1"
Oct 10 15:14:11 minikube kubelet[2509]: E1010 15:14:11.765583    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kubernetes-dashboard_kubernetes-dashboard-695b96c756-j4722_aebcc8a3-c744-4b8e-97c2-8961f665911b/kubernetes-dashboard/0.log\": failed to reopen container log \"56726cb95a84637cd552c692961dea3fee98c6e2fa86318332c9c6fa6be652d1\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="56726cb95a84637cd552c692961dea3fee98c6e2fa86318332c9c6fa6be652d1" path="/var/log/pods/kubernetes-dashboard_kubernetes-dashboard-695b96c756-j4722_aebcc8a3-c744-4b8e-97c2-8961f665911b/kubernetes-dashboard/0.log" currentSize=30240594 maxSize=10485760
Oct 10 15:14:11 minikube kubelet[2509]: E1010 15:14:11.769485    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="61a77a251dde39a26bc741a87e9f11e44eea79d35296163ac6c431221a177c59"
Oct 10 15:14:11 minikube kubelet[2509]: E1010 15:14:11.769519    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-6f6b679f8f-mmkcb_d99b555e-eaf8-441f-8098-c996824e5048/coredns/0.log\": failed to reopen container log \"61a77a251dde39a26bc741a87e9f11e44eea79d35296163ac6c431221a177c59\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="61a77a251dde39a26bc741a87e9f11e44eea79d35296163ac6c431221a177c59" path="/var/log/pods/kube-system_coredns-6f6b679f8f-mmkcb_d99b555e-eaf8-441f-8098-c996824e5048/coredns/0.log" currentSize=40472222 maxSize=10485760
Oct 10 15:14:18 minikube kubelet[2509]: I1010 15:14:18.176464    2509 helpers.go:943] "Eviction manager: no observation found for eviction signal" signal="containerfs.inodesFree"
Oct 10 15:14:18 minikube kubelet[2509]: I1010 15:14:18.176479    2509 helpers.go:943] "Eviction manager: no observation found for eviction signal" signal="containerfs.inodesFree"
Oct 10 15:14:21 minikube kubelet[2509]: E1010 15:14:21.766541    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8b69f8e1699b5ec0a60940f20f6786ec4ca6f72316bb8571d80285ebfbc48668"
Oct 10 15:14:21 minikube kubelet[2509]: E1010 15:14:21.766576    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/ipr_api-66bdd48d7b-fbkg8_31e548dd-a516-41a7-bb34-5bc8161ea278/ipr-api/0.log\": failed to reopen container log \"8b69f8e1699b5ec0a60940f20f6786ec4ca6f72316bb8571d80285ebfbc48668\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8b69f8e1699b5ec0a60940f20f6786ec4ca6f72316bb8571d80285ebfbc48668" path="/var/log/pods/ipr_api-66bdd48d7b-fbkg8_31e548dd-a516-41a7-bb34-5bc8161ea278/ipr-api/0.log" currentSize=35558327 maxSize=10485760
Oct 10 15:14:21 minikube kubelet[2509]: E1010 15:14:21.767261    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="0b7a4ecbc0fd900d4916217b80566c046490e9117460ce356facba654c7c828c"
Oct 10 15:14:21 minikube kubelet[2509]: E1010 15:14:21.767290    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kubernetes-dashboard_dashboard-metrics-scraper-c5db448b4-bwpkn_9c4dbafb-b0e2-4d96-b8c2-e805d495426d/dashboard-metrics-scraper/0.log\": failed to reopen container log \"0b7a4ecbc0fd900d4916217b80566c046490e9117460ce356facba654c7c828c\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="0b7a4ecbc0fd900d4916217b80566c046490e9117460ce356facba654c7c828c" path="/var/log/pods/kubernetes-dashboard_dashboard-metrics-scraper-c5db448b4-bwpkn_9c4dbafb-b0e2-4d96-b8c2-e805d495426d/dashboard-metrics-scraper/0.log" currentSize=15373254 maxSize=10485760
Oct 10 15:14:21 minikube kubelet[2509]: E1010 15:14:21.767954    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="56726cb95a84637cd552c692961dea3fee98c6e2fa86318332c9c6fa6be652d1"
Oct 10 15:14:21 minikube kubelet[2509]: E1010 15:14:21.767985    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kubernetes-dashboard_kubernetes-dashboard-695b96c756-j4722_aebcc8a3-c744-4b8e-97c2-8961f665911b/kubernetes-dashboard/0.log\": failed to reopen container log \"56726cb95a84637cd552c692961dea3fee98c6e2fa86318332c9c6fa6be652d1\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="56726cb95a84637cd552c692961dea3fee98c6e2fa86318332c9c6fa6be652d1" path="/var/log/pods/kubernetes-dashboard_kubernetes-dashboard-695b96c756-j4722_aebcc8a3-c744-4b8e-97c2-8961f665911b/kubernetes-dashboard/0.log" currentSize=30267643 maxSize=10485760
Oct 10 15:14:21 minikube kubelet[2509]: E1010 15:14:21.772044    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="61a77a251dde39a26bc741a87e9f11e44eea79d35296163ac6c431221a177c59"
Oct 10 15:14:21 minikube kubelet[2509]: E1010 15:14:21.772090    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-6f6b679f8f-mmkcb_d99b555e-eaf8-441f-8098-c996824e5048/coredns/0.log\": failed to reopen container log \"61a77a251dde39a26bc741a87e9f11e44eea79d35296163ac6c431221a177c59\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="61a77a251dde39a26bc741a87e9f11e44eea79d35296163ac6c431221a177c59" path="/var/log/pods/kube-system_coredns-6f6b679f8f-mmkcb_d99b555e-eaf8-441f-8098-c996824e5048/coredns/0.log" currentSize=40472222 maxSize=10485760
Oct 10 15:14:28 minikube kubelet[2509]: I1010 15:14:28.192138    2509 helpers.go:943] "Eviction manager: no observation found for eviction signal" signal="containerfs.inodesFree"
Oct 10 15:14:28 minikube kubelet[2509]: I1010 15:14:28.192152    2509 helpers.go:943] "Eviction manager: no observation found for eviction signal" signal="containerfs.inodesFree"
Oct 10 15:14:31 minikube kubelet[2509]: E1010 15:14:31.770029    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8b69f8e1699b5ec0a60940f20f6786ec4ca6f72316bb8571d80285ebfbc48668"
Oct 10 15:14:31 minikube kubelet[2509]: E1010 15:14:31.770081    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/ipr_api-66bdd48d7b-fbkg8_31e548dd-a516-41a7-bb34-5bc8161ea278/ipr-api/0.log\": failed to reopen container log \"8b69f8e1699b5ec0a60940f20f6786ec4ca6f72316bb8571d80285ebfbc48668\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8b69f8e1699b5ec0a60940f20f6786ec4ca6f72316bb8571d80285ebfbc48668" path="/var/log/pods/ipr_api-66bdd48d7b-fbkg8_31e548dd-a516-41a7-bb34-5bc8161ea278/ipr-api/0.log" currentSize=35558327 maxSize=10485760
Oct 10 15:14:31 minikube kubelet[2509]: E1010 15:14:31.770858    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="0b7a4ecbc0fd900d4916217b80566c046490e9117460ce356facba654c7c828c"
Oct 10 15:14:31 minikube kubelet[2509]: E1010 15:14:31.770896    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kubernetes-dashboard_dashboard-metrics-scraper-c5db448b4-bwpkn_9c4dbafb-b0e2-4d96-b8c2-e805d495426d/dashboard-metrics-scraper/0.log\": failed to reopen container log \"0b7a4ecbc0fd900d4916217b80566c046490e9117460ce356facba654c7c828c\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="0b7a4ecbc0fd900d4916217b80566c046490e9117460ce356facba654c7c828c" path="/var/log/pods/kubernetes-dashboard_dashboard-metrics-scraper-c5db448b4-bwpkn_9c4dbafb-b0e2-4d96-b8c2-e805d495426d/dashboard-metrics-scraper/0.log" currentSize=15397306 maxSize=10485760
Oct 10 15:14:31 minikube kubelet[2509]: E1010 15:14:31.771574    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="56726cb95a84637cd552c692961dea3fee98c6e2fa86318332c9c6fa6be652d1"
Oct 10 15:14:31 minikube kubelet[2509]: E1010 15:14:31.771605    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kubernetes-dashboard_kubernetes-dashboard-695b96c756-j4722_aebcc8a3-c744-4b8e-97c2-8961f665911b/kubernetes-dashboard/0.log\": failed to reopen container log \"56726cb95a84637cd552c692961dea3fee98c6e2fa86318332c9c6fa6be652d1\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="56726cb95a84637cd552c692961dea3fee98c6e2fa86318332c9c6fa6be652d1" path="/var/log/pods/kubernetes-dashboard_kubernetes-dashboard-695b96c756-j4722_aebcc8a3-c744-4b8e-97c2-8961f665911b/kubernetes-dashboard/0.log" currentSize=30294679 maxSize=10485760
Oct 10 15:14:31 minikube kubelet[2509]: E1010 15:14:31.775412    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="61a77a251dde39a26bc741a87e9f11e44eea79d35296163ac6c431221a177c59"
Oct 10 15:14:31 minikube kubelet[2509]: E1010 15:14:31.775446    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-6f6b679f8f-mmkcb_d99b555e-eaf8-441f-8098-c996824e5048/coredns/0.log\": failed to reopen container log \"61a77a251dde39a26bc741a87e9f11e44eea79d35296163ac6c431221a177c59\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="61a77a251dde39a26bc741a87e9f11e44eea79d35296163ac6c431221a177c59" path="/var/log/pods/kube-system_coredns-6f6b679f8f-mmkcb_d99b555e-eaf8-441f-8098-c996824e5048/coredns/0.log" currentSize=40478316 maxSize=10485760
Oct 10 15:14:38 minikube kubelet[2509]: I1010 15:14:38.204525    2509 helpers.go:943] "Eviction manager: no observation found for eviction signal" signal="containerfs.inodesFree"
Oct 10 15:14:38 minikube kubelet[2509]: I1010 15:14:38.204537    2509 helpers.go:943] "Eviction manager: no observation found for eviction signal" signal="containerfs.inodesFree"
Oct 10 15:14:41 minikube kubelet[2509]: E1010 15:14:41.772215    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8b69f8e1699b5ec0a60940f20f6786ec4ca6f72316bb8571d80285ebfbc48668"
Oct 10 15:14:41 minikube kubelet[2509]: E1010 15:14:41.772257    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/ipr_api-66bdd48d7b-fbkg8_31e548dd-a516-41a7-bb34-5bc8161ea278/ipr-api/0.log\": failed to reopen container log \"8b69f8e1699b5ec0a60940f20f6786ec4ca6f72316bb8571d80285ebfbc48668\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8b69f8e1699b5ec0a60940f20f6786ec4ca6f72316bb8571d80285ebfbc48668" path="/var/log/pods/ipr_api-66bdd48d7b-fbkg8_31e548dd-a516-41a7-bb34-5bc8161ea278/ipr-api/0.log" currentSize=35565461 maxSize=10485760
Oct 10 15:14:41 minikube kubelet[2509]: E1010 15:14:41.772875    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="0b7a4ecbc0fd900d4916217b80566c046490e9117460ce356facba654c7c828c"
Oct 10 15:14:41 minikube kubelet[2509]: E1010 15:14:41.772909    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kubernetes-dashboard_dashboard-metrics-scraper-c5db448b4-bwpkn_9c4dbafb-b0e2-4d96-b8c2-e805d495426d/dashboard-metrics-scraper/0.log\": failed to reopen container log \"0b7a4ecbc0fd900d4916217b80566c046490e9117460ce356facba654c7c828c\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="0b7a4ecbc0fd900d4916217b80566c046490e9117460ce356facba654c7c828c" path="/var/log/pods/kubernetes-dashboard_dashboard-metrics-scraper-c5db448b4-bwpkn_9c4dbafb-b0e2-4d96-b8c2-e805d495426d/dashboard-metrics-scraper/0.log" currentSize=15421603 maxSize=10485760
Oct 10 15:14:41 minikube kubelet[2509]: E1010 15:14:41.773501    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="56726cb95a84637cd552c692961dea3fee98c6e2fa86318332c9c6fa6be652d1"
Oct 10 15:14:41 minikube kubelet[2509]: E1010 15:14:41.773534    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kubernetes-dashboard_kubernetes-dashboard-695b96c756-j4722_aebcc8a3-c744-4b8e-97c2-8961f665911b/kubernetes-dashboard/0.log\": failed to reopen container log \"56726cb95a84637cd552c692961dea3fee98c6e2fa86318332c9c6fa6be652d1\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="56726cb95a84637cd552c692961dea3fee98c6e2fa86318332c9c6fa6be652d1" path="/var/log/pods/kubernetes-dashboard_kubernetes-dashboard-695b96c756-j4722_aebcc8a3-c744-4b8e-97c2-8961f665911b/kubernetes-dashboard/0.log" currentSize=30321726 maxSize=10485760
Oct 10 15:14:41 minikube kubelet[2509]: E1010 15:14:41.777125    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="61a77a251dde39a26bc741a87e9f11e44eea79d35296163ac6c431221a177c59"
Oct 10 15:14:41 minikube kubelet[2509]: E1010 15:14:41.777155    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-6f6b679f8f-mmkcb_d99b555e-eaf8-441f-8098-c996824e5048/coredns/0.log\": failed to reopen container log \"61a77a251dde39a26bc741a87e9f11e44eea79d35296163ac6c431221a177c59\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="61a77a251dde39a26bc741a87e9f11e44eea79d35296163ac6c431221a177c59" path="/var/log/pods/kube-system_coredns-6f6b679f8f-mmkcb_d99b555e-eaf8-441f-8098-c996824e5048/coredns/0.log" currentSize=40478316 maxSize=10485760
Oct 10 15:14:48 minikube kubelet[2509]: I1010 15:14:48.216530    2509 helpers.go:943] "Eviction manager: no observation found for eviction signal" signal="containerfs.inodesFree"
Oct 10 15:14:48 minikube kubelet[2509]: I1010 15:14:48.216542    2509 helpers.go:943] "Eviction manager: no observation found for eviction signal" signal="containerfs.inodesFree"
Oct 10 15:14:51 minikube kubelet[2509]: E1010 15:14:51.774870    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="8b69f8e1699b5ec0a60940f20f6786ec4ca6f72316bb8571d80285ebfbc48668"
Oct 10 15:14:51 minikube kubelet[2509]: E1010 15:14:51.774908    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/ipr_api-66bdd48d7b-fbkg8_31e548dd-a516-41a7-bb34-5bc8161ea278/ipr-api/0.log\": failed to reopen container log \"8b69f8e1699b5ec0a60940f20f6786ec4ca6f72316bb8571d80285ebfbc48668\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="8b69f8e1699b5ec0a60940f20f6786ec4ca6f72316bb8571d80285ebfbc48668" path="/var/log/pods/ipr_api-66bdd48d7b-fbkg8_31e548dd-a516-41a7-bb34-5bc8161ea278/ipr-api/0.log" currentSize=35565461 maxSize=10485760
Oct 10 15:14:51 minikube kubelet[2509]: E1010 15:14:51.775618    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="0b7a4ecbc0fd900d4916217b80566c046490e9117460ce356facba654c7c828c"
Oct 10 15:14:51 minikube kubelet[2509]: E1010 15:14:51.775648    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kubernetes-dashboard_dashboard-metrics-scraper-c5db448b4-bwpkn_9c4dbafb-b0e2-4d96-b8c2-e805d495426d/dashboard-metrics-scraper/0.log\": failed to reopen container log \"0b7a4ecbc0fd900d4916217b80566c046490e9117460ce356facba654c7c828c\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="0b7a4ecbc0fd900d4916217b80566c046490e9117460ce356facba654c7c828c" path="/var/log/pods/kubernetes-dashboard_dashboard-metrics-scraper-c5db448b4-bwpkn_9c4dbafb-b0e2-4d96-b8c2-e805d495426d/dashboard-metrics-scraper/0.log" currentSize=15445824 maxSize=10485760
Oct 10 15:14:51 minikube kubelet[2509]: E1010 15:14:51.776330    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="56726cb95a84637cd552c692961dea3fee98c6e2fa86318332c9c6fa6be652d1"
Oct 10 15:14:51 minikube kubelet[2509]: E1010 15:14:51.776353    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kubernetes-dashboard_kubernetes-dashboard-695b96c756-j4722_aebcc8a3-c744-4b8e-97c2-8961f665911b/kubernetes-dashboard/0.log\": failed to reopen container log \"56726cb95a84637cd552c692961dea3fee98c6e2fa86318332c9c6fa6be652d1\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="56726cb95a84637cd552c692961dea3fee98c6e2fa86318332c9c6fa6be652d1" path="/var/log/pods/kubernetes-dashboard_kubernetes-dashboard-695b96c756-j4722_aebcc8a3-c744-4b8e-97c2-8961f665911b/kubernetes-dashboard/0.log" currentSize=30348767 maxSize=10485760
Oct 10 15:14:51 minikube kubelet[2509]: E1010 15:14:51.780413    2509 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="61a77a251dde39a26bc741a87e9f11e44eea79d35296163ac6c431221a177c59"
Oct 10 15:14:51 minikube kubelet[2509]: E1010 15:14:51.780437    2509 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-6f6b679f8f-mmkcb_d99b555e-eaf8-441f-8098-c996824e5048/coredns/0.log\": failed to reopen container log \"61a77a251dde39a26bc741a87e9f11e44eea79d35296163ac6c431221a177c59\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="61a77a251dde39a26bc741a87e9f11e44eea79d35296163ac6c431221a177c59" path="/var/log/pods/kube-system_coredns-6f6b679f8f-mmkcb_d99b555e-eaf8-441f-8098-c996824e5048/coredns/0.log" currentSize=40478316 maxSize=10485760


==> kubernetes-dashboard [56726cb95a84] <==
2024/10/10 15:14:47 Getting list of all pet sets in the cluster
2024/10/10 15:14:47 received 0 resources from sidecar instead of 2
2024/10/10 15:14:47 received 0 resources from sidecar instead of 1
2024/10/10 15:14:47 received 0 resources from sidecar instead of 1
2024/10/10 15:14:47 received 0 resources from sidecar instead of 1
2024/10/10 15:14:47 received 0 resources from sidecar instead of 1
2024/10/10 15:14:47 received 0 resources from sidecar instead of 3
2024/10/10 15:14:47 received 0 resources from sidecar instead of 1
2024/10/10 15:14:47 received 0 resources from sidecar instead of 2
2024/10/10 15:14:47 [2024-10-10T15:14:47Z] Outcoming response to 127.0.0.1 with 200 status code
2024/10/10 15:14:47 received 0 resources from sidecar instead of 1
2024/10/10 15:14:47 received 0 resources from sidecar instead of 1
2024/10/10 15:14:47 received 0 resources from sidecar instead of 1
2024/10/10 15:14:47 received 0 resources from sidecar instead of 1
2024/10/10 15:14:47 received 0 resources from sidecar instead of 3
2024/10/10 15:14:47 received 0 resources from sidecar instead of 7
2024/10/10 15:14:47 received 0 resources from sidecar instead of 2
2024/10/10 15:14:47 received 0 resources from sidecar instead of 2
2024/10/10 15:14:47 Getting pod metrics
2024/10/10 15:14:47 [2024-10-10T15:14:47Z] Outcoming response to 127.0.0.1 with 200 status code
2024/10/10 15:14:47 received 0 resources from sidecar instead of 3
2024/10/10 15:14:47 received 0 resources from sidecar instead of 1
2024/10/10 15:14:47 received 0 resources from sidecar instead of 1
2024/10/10 15:14:47 received 0 resources from sidecar instead of 1
2024/10/10 15:14:47 [2024-10-10T15:14:47Z] Outcoming response to 127.0.0.1 with 200 status code
2024/10/10 15:14:47 [2024-10-10T15:14:47Z] Outcoming response to 127.0.0.1 with 200 status code
2024/10/10 15:14:47 received 0 resources from sidecar instead of 1
2024/10/10 15:14:47 received 0 resources from sidecar instead of 3
2024/10/10 15:14:47 received 0 resources from sidecar instead of 3
2024/10/10 15:14:47 received 0 resources from sidecar instead of 2
2024/10/10 15:14:47 received 0 resources from sidecar instead of 1
2024/10/10 15:14:47 received 0 resources from sidecar instead of 1
2024/10/10 15:14:47 received 0 resources from sidecar instead of 3
2024/10/10 15:14:47 received 0 resources from sidecar instead of 1
2024/10/10 15:14:47 received 0 resources from sidecar instead of 2
2024/10/10 15:14:47 received 0 resources from sidecar instead of 3
2024/10/10 15:14:47 Skipping metric because of error: Metric label not set.
2024/10/10 15:14:47 Skipping metric because of error: Metric label not set.
2024/10/10 15:14:47 Skipping metric because of error: Metric label not set.
2024/10/10 15:14:47 Skipping metric because of error: Metric label not set.
2024/10/10 15:14:47 Skipping metric because of error: Metric label not set.
2024/10/10 15:14:47 Skipping metric because of error: Metric label not set.
2024/10/10 15:14:47 Skipping metric because of error: Metric label not set.
2024/10/10 15:14:47 Skipping metric because of error: Metric label not set.
2024/10/10 15:14:47 Skipping metric because of error: Metric label not set.
2024/10/10 15:14:47 Skipping metric because of error: Metric label not set.
2024/10/10 15:14:47 Skipping metric because of error: Metric label not set.
2024/10/10 15:14:47 Skipping metric because of error: Metric label not set.
2024/10/10 15:14:47 Skipping metric because of error: Metric label not set.
2024/10/10 15:14:47 Skipping metric because of error: Metric label not set.
2024/10/10 15:14:47 Skipping metric because of error: Metric label not set.
2024/10/10 15:14:47 Skipping metric because of error: Metric label not set.
2024/10/10 15:14:47 Skipping metric because of error: Metric label not set.
2024/10/10 15:14:47 Skipping metric because of error: Metric label not set.
2024/10/10 15:14:47 Skipping metric because of error: Metric label not set.
2024/10/10 15:14:47 Skipping metric because of error: Metric label not set.
2024/10/10 15:14:47 [2024-10-10T15:14:47Z] Outcoming response to 127.0.0.1 with 200 status code
2024/10/10 15:14:51 [2024-10-10T15:14:51Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/10/10 15:14:51 Getting list of namespaces
2024/10/10 15:14:51 [2024-10-10T15:14:51Z] Outcoming response to 127.0.0.1 with 200 status code


==> storage-provisioner [53c9a9393261] <==
I1008 21:28:39.959106       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1008 21:29:09.960976       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [8471cd981887] <==
I1008 21:29:11.809264       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1008 21:29:11.813219       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1008 21:29:11.813249       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1008 21:29:11.867683       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1008 21:29:11.867752       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_25c0b27e-11e5-4f47-ae18-21dc8ad82024!
I1008 21:29:11.867725       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"0d4618f3-c711-48f7-8daa-2c1b7ca19423", APIVersion:"v1", ResourceVersion:"424", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_25c0b27e-11e5-4f47-ae18-21dc8ad82024 became leader
I1008 21:29:11.968026       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_25c0b27e-11e5-4f47-ae18-21dc8ad82024!

